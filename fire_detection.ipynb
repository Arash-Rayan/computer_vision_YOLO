{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e7d2178b",
   "metadata": {},
   "source": [
    "# points ! code is in progress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e243ff58",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch \n",
    "import cv2 as cv\n",
    "import os \n",
    "import matplotlib.pyplot as plt\n",
    "from torchinfo import summary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dd3fa889",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Literal \n",
    "\n",
    "def capture_vide_frames(video_path:str  , video_name:str, number_of_frames:int  , class_label:Literal[0, 1] =0 ): \n",
    "\n",
    "    # capturing frames \n",
    "    video_path = os.path.join(video_path , video_name)\n",
    "    cap = cv.VideoCapture(video_path)\n",
    "    \n",
    "    total_frames = cap.get(cv.CAP_PROP_FRAME_COUNT)\n",
    "    frame_indices = np.random.uniform(1, total_frames , number_of_frames).astype('int')\n",
    "    count_frames = 0 \n",
    "    img_sample = []\n",
    "\n",
    "    while cap : \n",
    "        isTrue, frame = cap.read()\n",
    "\n",
    "        if not isTrue : \n",
    "            break\n",
    "\n",
    "        count_frames+=1 \n",
    "\n",
    "        if count_frames in frame_indices: \n",
    "            img_sample.append(frame)\n",
    "\n",
    "        if cv.waitKey(10) & 0xff == ord('q'): \n",
    "            break\n",
    "        \n",
    "    cap.release()\n",
    "    cv.destroyAllWindows()\n",
    "\n",
    "    #optimizing the frames \n",
    "    resized_images = [cv.resize(img, (224, 224)) for img in img_sample]\n",
    "\n",
    "    #saving the frames \n",
    "    for idx,  img in enumerate(resized_images): \n",
    "\n",
    "        if class_label : \n",
    "            path = os.path.join('model_data/fire', video_name)\n",
    "            os.makedirs(path, exist_ok=True)\n",
    "            fileName= f'{idx}.jpg'\n",
    "            cv.imwrite(os.path.join(path, fileName) , img)\n",
    "\n",
    "        else : \n",
    "            path = os.path.join('model_data/no_fire', video_name)\n",
    "            os.makedirs(path , exist_ok=True)\n",
    "            fileName = f'{idx}.jpg'\n",
    "            cv.imwrite(os.path.join(path, fileName) , img)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d7fe3784",
   "metadata": {},
   "outputs": [],
   "source": [
    "fire_vidoes_list  = os.listdir(os.path.join('data/videos/fire'))\n",
    "no_fire_video_list  = os.listdir(os.path.join('data/videos/noFire'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5b4d9ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "for vid in fire_vidoes_list: \n",
    "    capture_vide_frames('data/videos/fire', vid, 40, 1)\n",
    "\n",
    "\n",
    "for vid in no_fire_video_list:\n",
    "    capture_vide_frames('data/videos/nofire' , vid , 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7f841b8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "278 225\n"
     ]
    }
   ],
   "source": [
    "import glob \n",
    "\n",
    "fire_images = glob.glob('model_data/fire/**/*')\n",
    "no_fire_images = glob.glob('model_data/no_fire/**/*')\n",
    "\n",
    "print(len(fire_images), len(no_fire_images))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a4acbe69",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader , random_split\n",
    "from PIL import Image\n",
    "from torchvision import transforms "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "da5b9eb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FireVideoDataset(Dataset):\n",
    "    def __init__(self, root_dir, chunk_size=17):\n",
    "        self.chunk_size = chunk_size\n",
    "        self.samples = []\n",
    "        self.transform = transforms.Compose([\n",
    "            transforms.ToTensor(),  # Convert PIL image to min max scale (0-1)\n",
    "            transforms.Normalize(\n",
    "                    [0.485, 0.456, 0.406],\n",
    "                    [0.229, 0.224, 0.225]\n",
    "                    ), \n",
    "            # transforms.RandomHorizontalFlip(0.5), \n",
    "            # transforms.ColorJitter(0.2)\n",
    "        ])\n",
    "\n",
    "        for class_name, label in [('fire', 1), ('no_fire', 0)]:\n",
    "            class_dir = os.path.join(root_dir, class_name)\n",
    "\n",
    "            for chunk_folder in os.listdir(class_dir):\n",
    "                chunk_path = os.path.join(class_dir, chunk_folder)\n",
    "                \n",
    "                if os.path.isdir(chunk_path) and len(os.listdir(chunk_path)) >= chunk_size:\n",
    "                    self.samples.append((chunk_path, label))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        chunk_path, label = self.samples[idx]\n",
    "        \n",
    "        image_files = os.listdir(chunk_path)[:self.chunk_size] \n",
    "        frames = []\n",
    "\n",
    "        for fname in image_files:\n",
    "            img_path = os.path.join(chunk_path, fname)\n",
    "            img = Image.open(img_path)\n",
    "\n",
    "            if img is None:\n",
    "                continue  \n",
    "            frames.append(self.transform(img)) \n",
    "        np_frame = np.array(frames)  # its [B , C , H , W]\n",
    "        np_frame = np.transpose(np_frame, (1 , 0, 2, 3))  # becomes [C , B , H , W]\n",
    "\n",
    "        return torch.tensor(np_frame, dtype=torch.float32), torch.tensor(label, dtype=torch.float32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a13e3f71",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = FireVideoDataset(root_dir='model_data')\n",
    "\n",
    "train_size = int(0.7 *len(dataset))\n",
    "test_size = len(dataset) - train_size\n",
    "\n",
    "train_dataset, test_dataset = random_split(dataset, [train_size, test_size])\n",
    "\n",
    "train_data_loader = DataLoader(train_dataset, batch_size=3 ,shuffle=True)\n",
    "test_data_loader = DataLoader(test_dataset, batch_size=3, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6013fc55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7 7\n"
     ]
    }
   ],
   "source": [
    "pos = 0 \n",
    "neg = 0 \n",
    "for i in dataset: \n",
    "    if  i[1] == 1 : \n",
    "        pos+=1 \n",
    "    else :\n",
    "       neg +=1 \n",
    "\n",
    "print(pos , neg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "487a62c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class FireDetector(nn.Module):\n",
    "    def __init__(self, in_channels=3, num_classes=1):\n",
    "        super().__init__()\n",
    "        self.conv_block = nn.Sequential(\n",
    "            nn.Conv3d(in_channels, 16, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm3d(16),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool3d((1, 2, 2)),  # frame , hight , width (keep fram , max pool each frame)\n",
    "\n",
    "            nn.Conv3d(16, 32, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm3d(32),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool3d((2, 2, 2)), # pick two frame , maxpool each frame \n",
    "\n",
    "            nn.Conv3d(32, 64, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm3d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.AdaptiveAvgPool3d((1, 1, 1))  # compress to (1,1,1) # pick one frame , one pixel(best one in hight and ..)\n",
    "        )\n",
    "        self.classifier = nn.Linear(64, num_classes) # [9, 64  ,  1 ,1 , 1] we resize to have channels giving to linear to pick 2 \n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv_block(x)\n",
    "        x = x.squeeze() # flatten\n",
    "        x = self.classifier(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "efa6c34f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "=================================================================\n",
       "Layer (type:depth-idx)                   Param #\n",
       "=================================================================\n",
       "FireDetector                             --\n",
       "├─Sequential: 1-1                        --\n",
       "│    └─Conv3d: 2-1                       1,312\n",
       "│    └─BatchNorm3d: 2-2                  32\n",
       "│    └─ReLU: 2-3                         --\n",
       "│    └─MaxPool3d: 2-4                    --\n",
       "│    └─Conv3d: 2-5                       13,856\n",
       "│    └─BatchNorm3d: 2-6                  64\n",
       "│    └─ReLU: 2-7                         --\n",
       "│    └─MaxPool3d: 2-8                    --\n",
       "│    └─Conv3d: 2-9                       55,360\n",
       "│    └─BatchNorm3d: 2-10                 128\n",
       "│    └─ReLU: 2-11                        --\n",
       "│    └─AdaptiveAvgPool3d: 2-12           --\n",
       "├─Linear: 1-2                            65\n",
       "=================================================================\n",
       "Total params: 70,817\n",
       "Trainable params: 70,817\n",
       "Non-trainable params: 0\n",
       "================================================================="
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fire_detector_model = FireDetector()\n",
    "summary(fire_detector_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "863d001b",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = nn.BCEWithLogitsLoss()\n",
    "def measure_acc(ypred , ytrue): \n",
    "\n",
    "    from torchmetrics import Accuracy , Recall\n",
    "    acc_fn = Accuracy(task='binary')\n",
    "    acc = acc_fn(ypred , ytrue)\n",
    "\n",
    "    sig_ypred = (torch.sigmoid(ypred) > 0.5).int()\n",
    "    recallfn = Recall(task='binary')\n",
    "    recall = recallfn(sig_ypred , ytrue)\n",
    "    return acc , recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "70ae6da3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Tuple\n",
    "from tqdm import tqdm \n",
    "\n",
    "def train_the_model(model , number_of_epochs:int ,\n",
    "                     data_loader:DataLoader, loss_fn:torch.nn.Module , \n",
    "                     ) -> Tuple[float, float] :\n",
    "    \n",
    "    optimizer = torch.optim.Adam(params=model.parameters() , lr=1e-1, weight_decay=1e-3)\n",
    "    \n",
    "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    model.to(device)\n",
    "    \n",
    "    for epoch in tqdm(range(number_of_epochs)): \n",
    "        model.train()\n",
    "        batch_loss = 0\n",
    "        batch_acc = 0 \n",
    "        batch_recall = 0\n",
    "\n",
    "        for x , y in data_loader : \n",
    "            ypred = model(x)\n",
    "            ypred = ypred.squeeze()\n",
    "            loss = loss_fn(ypred, y)\n",
    "            batch_loss += loss.item() \n",
    "            acc , recall = measure_acc(ypred, y) \n",
    "            batch_acc += acc.item()\n",
    "            batch_recall += recall.item()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        batch_loss /= len(data_loader)\n",
    "        batch_acc /= len(data_loader)\n",
    "        batch_recall /= len(data_loader)\n",
    "    return batch_loss , batch_acc, batch_recall\n",
    "\n",
    "\n",
    "def eval_the_model(model:torch.nn.Module ,data_loader:DataLoader, loss_fn:torch.nn.Module ):\n",
    "    test_loss = 0 \n",
    "    test_acc = 0 \n",
    "    batch_recall = 0\n",
    "\n",
    "    model.eval()\n",
    "    with torch.inference_mode(): \n",
    "        for x , y in data_loader: \n",
    "            ypred = model(x)\n",
    "            ypred = np.squeeze(ypred)\n",
    "            loss = loss_fn(ypred , y)\n",
    "            test_loss+=loss.item()\n",
    "            acc , recall = measure_acc(ypred , y)\n",
    "            test_acc += acc.item()\n",
    "            batch_recall += recall.item()\n",
    "        test_loss /= len(data_loader)\n",
    "        test_acc /= len(data_loader)\n",
    "        batch_recall /= len(data_loader)\n",
    "    \n",
    "    return test_loss, test_acc , batch_recall\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3e6826c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:36<00:00,  3.64s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss is 1.308441\n",
      " train accuracy is 0.78\n",
      " recall is 1.00\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "train_loss , train_acc , recall= train_the_model(fire_detector_model , 10 , train_data_loader , loss_fn  )\n",
    "print(f'train loss is {train_loss:2f}\\n',\n",
    "       f'train accuracy is {train_acc:.2f}\\n', \n",
    "       f'recall is {recall:.2f}\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4b37936b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss is 1.308441\n",
      " train accuracy is 0.78\n",
      " recall is 0.00\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_loss , test_acc , recall = eval_the_model(fire_detector_model  , test_data_loader , loss_fn)\n",
    "print(f'train loss is {train_loss:2f}\\n',\n",
    "       f'train accuracy is {train_acc:.2f}\\n', \n",
    "       f'recall is {recall:.2f}\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3674a598",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Linear(in_features=512, out_features=1, bias=True)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torchvision.models.video import r3d_18 , R3D_18_Weights\n",
    "resnet_model = r3d_18(weights=R3D_18_Weights.DEFAULT)\n",
    "resnet_model.fc = nn.Linear(resnet_model.fc.in_features, 1)  # Binary classification\n",
    "\n",
    "resnet_model.fc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "827b7b69",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:50<00:00, 50.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss is 7.387102\n",
      " train accuracy is 0.44\n",
      " recall is 0.67\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "train_loss , train_acc , recall= train_the_model(resnet_model , 1, train_data_loader , loss_fn  )\n",
    "print(f'train loss is {train_loss:2f}\\n',\n",
    "       f'train accuracy is {train_acc:.2f}\\n', \n",
    "       f'recall is {recall:.2f}\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "eeade5ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test loss is 7615515228897280.0\n",
      " test accuracy is 0.4166666716337204\n",
      " recall is 1.0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_loss , test_acc , recall = eval_the_model(resnet_model ,  test_data_loader, loss_fn)\n",
    "print(f'test loss is {test_loss}\\n',\n",
    "       f'test accuracy is {test_acc}\\n', \n",
    "       f'recall is {recall}\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaa8ffec",
   "metadata": {},
   "source": [
    "### trying conv2dand1D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8891a98b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FireDetectorConv2D1D(nn.Module): \n",
    "    def __init__(self, in_channels = 3 , num_classes=1): \n",
    "        super().__init__() \n",
    "        self.spatial_extractor = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, 32, kernel_size = 3, padding=1),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(), \n",
    "            nn.MaxPool2d(2),\n",
    "\n",
    "            nn.Conv2d(32 , 64, kernel_size = 3 , padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.AdaptiveAvgPool2d(1),\n",
    "        )\n",
    "\n",
    "        self.temporal_extractor = nn.Sequential(\n",
    "            nn.Conv1d(64, 128 , 3 ,padding = 1 ),\n",
    "            nn.BatchNorm1d(128), \n",
    "            nn.ReLU(), \n",
    "            nn.AdaptiveAvgPool1d(1) \n",
    "        )\n",
    "\n",
    "        self.clasifier = nn.Linear(128, num_classes)\n",
    "\n",
    "    def forward(self , x): \n",
    "            B , C , T , H , W = x.size()\n",
    "            x = x.permute(0, 2, 1, 3 ,4)  # (B, T, C, H, W)\n",
    "            x = x.reshape(B * T , C , H , W)\n",
    "            x = self.spatial_extractor(x) # (B*T, 64, 1, 1)\n",
    "            x = x.view(B , T , 64)  # (B, T, 64)\n",
    "\n",
    "            x = x.permute(0 , 2 , 1) # (B , 64, T)\n",
    "            x = self.temporal_extractor(x) # (B , 128, 1)\n",
    "            x = x.squeeze(-1) # (B , 128)\n",
    "\n",
    "            return self.clasifier(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3f1132c2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "=================================================================\n",
       "Layer (type:depth-idx)                   Param #\n",
       "=================================================================\n",
       "FireDetectorConv2D1D                     --\n",
       "├─Sequential: 1-1                        --\n",
       "│    └─Conv2d: 2-1                       896\n",
       "│    └─BatchNorm2d: 2-2                  64\n",
       "│    └─ReLU: 2-3                         --\n",
       "│    └─MaxPool2d: 2-4                    --\n",
       "│    └─Conv2d: 2-5                       18,496\n",
       "│    └─BatchNorm2d: 2-6                  128\n",
       "│    └─ReLU: 2-7                         --\n",
       "│    └─AdaptiveAvgPool2d: 2-8            --\n",
       "├─Sequential: 1-2                        --\n",
       "│    └─Conv1d: 2-9                       24,704\n",
       "│    └─BatchNorm1d: 2-10                 256\n",
       "│    └─ReLU: 2-11                        --\n",
       "│    └─AdaptiveAvgPool1d: 2-12           --\n",
       "├─Linear: 1-3                            129\n",
       "=================================================================\n",
       "Total params: 44,673\n",
       "Trainable params: 44,673\n",
       "Non-trainable params: 0\n",
       "================================================================="
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model2d_plus = FireDetectorConv2D1D()\n",
    "summary(model2d_plus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1c6160bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 15/15 [00:52<00:00,  3.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss is 1.114936\n",
      " train accuracy is 0.89\n",
      " recall is 0.89\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "train_loss , train_acc , recall = train_the_model(model2d_plus, 15 , train_data_loader , loss_fn )\n",
    "print(f'train loss is {train_loss:2f}\\n',\n",
    "       f'train accuracy is {train_acc:.2f}\\n', \n",
    "       f'recall is {recall:.2f}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "bf85b208",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test loss is 34.941568\n",
      " test accuracy is 0.00\n",
      " recall is 0.00\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_loss , test_acc , recall = eval_the_model(model2d_plus, test_data_loader, loss_fn )\n",
    "print(f'test loss is {test_loss:2f}\\n',\n",
    "       f'test accuracy is {test_acc:.2f}\\n', \n",
    "       f'recall is {recall:.2f}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a560f6e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FireDetectorMain3D(nn.Module): \n",
    "    def __init__(self, in_channels , out_channels, kernel_size, padding): \n",
    "        super().__init__()\n",
    "        T, H, W = kernel_size\n",
    "\n",
    "        self.spatial_conv = nn.Conv3d(\n",
    "            in_channels,  out_channels , kernel_size=(1 , H, W), padding=(0, padding , padding),\n",
    "        )\n",
    "\n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "        self.temporal_conv = nn.Conv3d(\n",
    "            out_channels , out_channels , kernel_size=(T , 1, 1) , padding=(padding, 0 , 0)\n",
    "        )\n",
    "\n",
    "    def forward(self, x): \n",
    "        x = self.spatial_conv(x)\n",
    "        x = self.relu(x) \n",
    "        x = self.temporal_conv(x) \n",
    "        return x \n",
    "\n",
    "class Residual(nn.Module): \n",
    "    def __init__(self, channels , kernel_size, padding =1):\n",
    "        super().__init__()\n",
    "        self.conv1 = FireDetectorMain3D(channels, channels, kernel_size , padding)\n",
    "        self.norm1 = nn.LayerNorm(channels)\n",
    "        self.conv2 = FireDetectorMain3D(channels ,channels, kernel_size , padding)\n",
    "        self.norm2 = nn.LayerNorm(channels)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x) : \n",
    "        Residual = x \n",
    "        out = self.conv1(x) #(N, C, D, H, W)\n",
    "        out = out.permute(0, 2 , 3 , 4 , 1)\n",
    "        out = self.norm1(out)\n",
    "        out= out.permute(0 , 4 , 1 , 2, 3)\n",
    "        # out = self.relu(out)\n",
    "\n",
    "        out = self.conv2(out)\n",
    "        out = out.permute(0, 2 , 3 , 4 , 1)\n",
    "        out = self.norm2(out)\n",
    "        out= out.permute(0 , 4 , 1 , 2, 3)\n",
    "\n",
    "        out += Residual\n",
    "        out = self.relu(out)\n",
    "        return out \n",
    "    \n",
    "class FireDetectorWithResidual(nn.Module): \n",
    "        def __init__(self , in_channels =3 , num_clases =1) :\n",
    "            super().__init__()\n",
    "            self.initial_conv = FireDetectorMain3D(in_channels, 16 , kernel_size=(3, 7, 7) , padding=1)\n",
    "            self.bn = nn.BatchNorm3d(16)\n",
    "            self.relu = nn.ReLU() \n",
    "            self.pool1 = nn.MaxPool3d((1 , 2 ,2))\n",
    "            self.res_block1 = Residual(16, kernel_size=(3 ,3 ,3))\n",
    "            self.pool2 = nn.MaxPool3d((2 ,2 ,2))\n",
    "\n",
    "            self.res_block2 = Residual(16, kernel_size=(3 ,3 ,3))\n",
    "            self.adaptive_pool = nn.AdaptiveAvgPool3d((1 ,1,1))\n",
    "\n",
    "            self.classifier = nn.Linear(16 , num_clases)\n",
    "\n",
    "        def forward(self  , x): \n",
    "            x = self.initial_conv(x)\n",
    "            x = self.bn(x)\n",
    "            x = self.relu(x)\n",
    "            x = self.pool1(x)\n",
    "\n",
    "            x = self.res_block1(x)\n",
    "            x = self.pool2(x)\n",
    "\n",
    "            x = self.res_block2(x)\n",
    "            x = self.adaptive_pool(x)\n",
    "\n",
    "            x = x.flatten(1)\n",
    "            x = self.classifier(x)\n",
    "\n",
    "            return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a9db1697",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_sample = next(iter(train_data_loader))[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c64e2824",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 3, 17, 224, 224])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_sample.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "4adc50d3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "==========================================================================================\n",
       "Layer (type:depth-idx)                   Output Shape              Param #\n",
       "==========================================================================================\n",
       "FireDetectorWithResidual                 [3, 1]                    --\n",
       "├─FireDetectorMain3D: 1-1                [3, 16, 17, 220, 220]     --\n",
       "│    └─Conv3d: 2-1                       [3, 16, 17, 220, 220]     2,368\n",
       "│    └─ReLU: 2-2                         [3, 16, 17, 220, 220]     --\n",
       "│    └─Conv3d: 2-3                       [3, 16, 17, 220, 220]     784\n",
       "├─BatchNorm3d: 1-2                       [3, 16, 17, 220, 220]     32\n",
       "├─ReLU: 1-3                              [3, 16, 17, 220, 220]     --\n",
       "├─MaxPool3d: 1-4                         [3, 16, 17, 110, 110]     --\n",
       "├─Residual: 1-5                          [3, 16, 17, 110, 110]     --\n",
       "│    └─FireDetectorMain3D: 2-4           [3, 16, 17, 110, 110]     --\n",
       "│    │    └─Conv3d: 3-1                  [3, 16, 17, 110, 110]     2,320\n",
       "│    │    └─ReLU: 3-2                    [3, 16, 17, 110, 110]     --\n",
       "│    │    └─Conv3d: 3-3                  [3, 16, 17, 110, 110]     784\n",
       "│    └─LayerNorm: 2-5                    [3, 17, 110, 110, 16]     32\n",
       "│    └─FireDetectorMain3D: 2-6           [3, 16, 17, 110, 110]     --\n",
       "│    │    └─Conv3d: 3-4                  [3, 16, 17, 110, 110]     2,320\n",
       "│    │    └─ReLU: 3-5                    [3, 16, 17, 110, 110]     --\n",
       "│    │    └─Conv3d: 3-6                  [3, 16, 17, 110, 110]     784\n",
       "│    └─LayerNorm: 2-7                    [3, 17, 110, 110, 16]     32\n",
       "│    └─ReLU: 2-8                         [3, 16, 17, 110, 110]     --\n",
       "├─MaxPool3d: 1-6                         [3, 16, 8, 55, 55]        --\n",
       "├─Residual: 1-7                          [3, 16, 8, 55, 55]        --\n",
       "│    └─FireDetectorMain3D: 2-9           [3, 16, 8, 55, 55]        --\n",
       "│    │    └─Conv3d: 3-7                  [3, 16, 8, 55, 55]        2,320\n",
       "│    │    └─ReLU: 3-8                    [3, 16, 8, 55, 55]        --\n",
       "│    │    └─Conv3d: 3-9                  [3, 16, 8, 55, 55]        784\n",
       "│    └─LayerNorm: 2-10                   [3, 8, 55, 55, 16]        32\n",
       "│    └─FireDetectorMain3D: 2-11          [3, 16, 8, 55, 55]        --\n",
       "│    │    └─Conv3d: 3-10                 [3, 16, 8, 55, 55]        2,320\n",
       "│    │    └─ReLU: 3-11                   [3, 16, 8, 55, 55]        --\n",
       "│    │    └─Conv3d: 3-12                 [3, 16, 8, 55, 55]        784\n",
       "│    └─LayerNorm: 2-12                   [3, 8, 55, 55, 16]        32\n",
       "│    └─ReLU: 2-13                        [3, 16, 8, 55, 55]        --\n",
       "├─AdaptiveAvgPool3d: 1-8                 [3, 16, 1, 1, 1]          --\n",
       "├─Linear: 1-9                            [3, 1]                    17\n",
       "==========================================================================================\n",
       "Total params: 15,745\n",
       "Trainable params: 15,745\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (Units.GIGABYTES): 12.06\n",
       "==========================================================================================\n",
       "Input size (MB): 30.71\n",
       "Forward/backward pass size (MB): 1477.56\n",
       "Params size (MB): 0.06\n",
       "Estimated Total Size (MB): 1508.33\n",
       "=========================================================================================="
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "resnet2d1d = FireDetectorWithResidual()\n",
    "summary(resnet2d1d, input_size=test_sample.shape)  # (batch, C, D, H, W)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "9fd1b5bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [02:37<00:00,  3.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss is 1.4138781825701396\n",
      " train accuracy is 0.4444444576899211\n",
      " recall is 0.8333333333333334\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "train_loss , train_acc, recall = train_the_model(resnet2d1d , 50 , train_data_loader, loss_fn )\n",
    "print(f'train loss is {train_loss}\\n',\n",
    "       f'train accuracy is {train_acc}\\n', \n",
    "       f'recall is {recall}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "a25708ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test loss is 2.0982104539871216\n",
      " test accuracy is 0.3333333432674408\n",
      " recall is 0.8333333333333334\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_loss, test_acc , test_recall = eval_the_model(resnet2d1d , test_data_loader, loss_fn)\n",
    "print(f'test loss is {test_loss}\\n',\n",
    "       f'test accuracy is {test_acc}\\n', \n",
    "       f'recall is {recall}\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32ef4c45",
   "metadata": {},
   "source": [
    "## slow fast model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "771ec5cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in C:\\Users\\alchemist/.cache\\torch\\hub\\facebookresearch_pytorchvideo_main\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "# Choose the `slowfast_r50` model \n",
    "slow_fast_model = torch.hub.load('facebookresearch/pytorchvideo', 'slowfast_r50', pretrained=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "2c76a06e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "=====================================================================================\n",
       "Layer (type:depth-idx)                                       Param #\n",
       "=====================================================================================\n",
       "Net                                                          --\n",
       "├─ModuleList: 1-1                                            --\n",
       "│    └─MultiPathWayWithFuse: 2-1                             --\n",
       "│    │    └─ModuleList: 3-1                                  15,432\n",
       "│    │    └─FuseFastToSlow: 3-2                              928\n",
       "│    └─MultiPathWayWithFuse: 2-2                             --\n",
       "│    │    └─ModuleList: 3-3                                  225,760\n",
       "│    │    └─FuseFastToSlow: 3-4                              14,464\n",
       "│    └─MultiPathWayWithFuse: 2-3                             --\n",
       "│    │    └─ModuleList: 3-5                                  1,287,552\n",
       "│    │    └─FuseFastToSlow: 3-6                              57,600\n",
       "│    └─MultiPathWayWithFuse: 2-4                             --\n",
       "│    │    └─ModuleList: 3-7                                  10,369,536\n",
       "│    │    └─FuseFastToSlow: 3-8                              229,888\n",
       "│    └─MultiPathWayWithFuse: 2-5                             --\n",
       "│    │    └─ModuleList: 3-9                                  21,443,328\n",
       "│    │    └─Identity: 3-10                                   --\n",
       "│    └─PoolConcatPathway: 2-6                                --\n",
       "│    │    └─ModuleList: 3-11                                 --\n",
       "│    └─ResNetBasicHead: 2-7                                  --\n",
       "│    │    └─Dropout: 3-12                                    --\n",
       "│    │    └─Linear: 3-13                                     922,000\n",
       "│    │    └─AdaptiveAvgPool3d: 3-14                          --\n",
       "=====================================================================================\n",
       "Total params: 34,566,488\n",
       "Trainable params: 34,566,488\n",
       "Non-trainable params: 0\n",
       "====================================================================================="
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary(slow_fast_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "214f1106",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\alchemist\\Desktop\\computer_vision\\venv\\Lib\\site-packages\\torchvision\\transforms\\_functional_video.py:6: UserWarning: The 'torchvision.transforms._functional_video' module is deprecated since 0.12 and will be removed in the future. Please use the 'torchvision.transforms.functional' module instead.\n",
      "  warnings.warn(\n",
      "c:\\Users\\alchemist\\Desktop\\computer_vision\\venv\\Lib\\site-packages\\torchvision\\transforms\\_transforms_video.py:22: UserWarning: The 'torchvision.transforms._transforms_video' module is deprecated since 0.12 and will be removed in the future. Please use the 'torchvision.transforms' module instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\alchemist/.cache\\torch\\hub\\facebookresearch_pytorchvideo_main\\pytorchvideo\\data\\frame_video.py:106: SyntaxWarning: invalid escape sequence '\\d'\n",
      "  return [int(c) if c.isdigit() else c for c in re.split(\"(\\d+)\", text)]\n"
     ]
    }
   ],
   "source": [
    "from typing import Dict\n",
    "import json\n",
    "import urllib\n",
    "from torchvision.transforms import Compose, Lambda\n",
    "from torchvision.transforms._transforms_video import (\n",
    "    CenterCropVideo,\n",
    "    NormalizeVideo,\n",
    ")\n",
    "from pytorchvideo.data.encoded_video import EncodedVideo\n",
    "from pytorchvideo.transforms import (\n",
    "    ApplyTransformToKey,\n",
    "    ShortSideScale,\n",
    "    UniformTemporalSubsample,\n",
    "    UniformCropVideo\n",
    ") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "98ed33ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cpu\"\n",
    "slow_fast_model = slow_fast_model.eval()\n",
    "slow_fast_model = slow_fast_model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc812b5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_loss , train_acc , recall = train_the_model(slow_fast_model,1 , train_data_loader , loss_fn , optimizer)\n",
    "# print(train_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0d71235",
   "metadata": {},
   "outputs": [],
   "source": [
    "# side_size = 256\n",
    "# mean = [0.45, 0.45, 0.45]\n",
    "# std = [0.225, 0.225, 0.225]\n",
    "# crop_size = 256\n",
    "# num_frames = 32\n",
    "# sampling_rate = 2\n",
    "# frames_per_second = 30\n",
    "# slowfast_alpha = 4\n",
    "# num_clips = 10\n",
    "# num_crops = 3\n",
    "\n",
    "# class PackPathway(torch.nn.Module):\n",
    "#     \"\"\"\n",
    "#     Transform for converting video frames as a list of tensors. \n",
    "#     \"\"\"\n",
    "#     def __init__(self):\n",
    "#         super().__init__()\n",
    "        \n",
    "#     def forward(self, frames: torch.Tensor):\n",
    "#         fast_pathway = frames\n",
    "#         # Perform temporal sampling from the fast pathway.\n",
    "#         slow_pathway = torch.index_select(\n",
    "#             frames,\n",
    "#             1,\n",
    "#             torch.linspace(\n",
    "#                 0, frames.shape[1] - 1, frames.shape[1] // slowfast_alpha\n",
    "#             ).long(),\n",
    "#         )\n",
    "#         frame_list = [slow_pathway, fast_pathway]\n",
    "#         return frame_list\n",
    "\n",
    "# transform =  ApplyTransformToKey(\n",
    "#     key=\"video\",\n",
    "#     transform=Compose(\n",
    "#         [\n",
    "#             UniformTemporalSubsample(num_frames),\n",
    "#             Lambda(lambda x: x/255.0),\n",
    "#             NormalizeVideo(mean, std),\n",
    "#             ShortSideScale(\n",
    "#                 size=side_size\n",
    "#             ),\n",
    "#             CenterCropVideo(crop_size),\n",
    "#             PackPathway()\n",
    "#         ]\n",
    "#     ),\n",
    "# )\n",
    "\n",
    "# # The duration of the input clip is also specific to the model.\n",
    "# clip_duration = (num_frames * sampling_rate)/frames_per_second"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "fe2fbd72",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict\n",
    "import json\n",
    "import urllib\n",
    "\n",
    "from torchvision.transforms import Compose, Lambda, CenterCrop, Normalize\n",
    "from pytorchvideo.data.encoded_video import EncodedVideo\n",
    "# Optional: if needed\n",
    "# from pytorchvideo.transforms import (\n",
    "#     ApplyTransformToKey,\n",
    "#     ShortSideScale,\n",
    "#     UniformTemporalSubsample,\n",
    "#     UniformCropVideo\n",
    "# )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e684c49",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 30, 720, 1280])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "video = EncodedVideo.from_path('data/videos/fire/fire_2.mp4')\n",
    "clip = video.get_clip(start_sec=0, end_sec=1.0)  # Extract a 2-second segment\n",
    "clip['video'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "d9a95868",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = ApplyTransformToKey(\n",
    "    key=\"video\",\n",
    "    transform=Compose([\n",
    "        UniformTemporalSubsample(32),\n",
    "        Lambda(lambda x: x / 255.0),\n",
    "        NormalizeVideo(mean=[0.45, 0.45, 0.45], std=[0.225, 0.225, 0.225]),\n",
    "        ShortSideScale(256),\n",
    "        CenterCropVideo(224),\n",
    "    ])\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "289bca24",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 30, 720, 1280])"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clip['video'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "8b5af7d5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 30, 720, 1280])"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "myclip = {'video' :  clip['video']}\n",
    "myclip['video'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "b60227da",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 3, 32, 224, 224])"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ts_image = transform(myclip)\n",
    "final_ts_image = ts_image['video'].unsqueeze(0)\n",
    "final_ts_image.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "a405a460",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pack_pathway_output(frames, alpha=4):\n",
    "    \"\"\"\n",
    "    Convert video to SlowFast input format:\n",
    "    - fast_pathway: the original (e.g. 30 frames)\n",
    "    - slow_pathway: subsampled by alpha (e.g. 30 // 4 = ~8 frames)\n",
    "    \"\"\"\n",
    "    fast_pathway = frames\n",
    "    slow_pathway = frames[:, :, ::alpha, :, :]  # temporal subsampling\n",
    "    return [slow_pathway, fast_pathway]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "a9782d09",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 3, 32, 224, 224])"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_ts_image.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "6e6aa194",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 3, 8, 224, 224]), torch.Size([1, 3, 32, 224, 224]))"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ready = pack_pathway_output(final_ts_image)\n",
    "ready[0].shape , ready[1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "defe10f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.inference_mode(): \n",
    "    output = slow_fast_model(ready)\n",
    "    output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "458c203c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 400])"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output.shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
