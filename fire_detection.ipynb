{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e7d2178b",
   "metadata": {},
   "source": [
    "# points ! code is in progress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e243ff58",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch \n",
    "import cv2 as cv\n",
    "import os \n",
    "import matplotlib.pyplot as plt \n",
    "from torchinfo import summary \n",
    "import torch.nn as nn\n",
    "from typing import Tuple , Callable , Optional\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd3fa889",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from typing import Literal \n",
    "\n",
    "# def capture_vide_frames(video_path:str  , video_name:str, \n",
    "#                         number_of_frames:int  , class_label:Literal[0, 1] =0 ): \n",
    "\n",
    "#     # capturing frames \n",
    "#     video_path = os.path.join(video_path , video_name)\n",
    "#     cap = cv.VideoCapture(video_path)\n",
    "    \n",
    "#     total_frames = cap.get(cv.CAP_PROP_FRAME_COUNT)\n",
    "#     frame_indices = np.random.uniform(1, total_frames , number_of_frames).astype('int')\n",
    "#     count_frames = 0 \n",
    "#     img_sample = []\n",
    "\n",
    "#     while cap : \n",
    "#         isTrue, frame = cap.read()\n",
    "\n",
    "#         if not isTrue : \n",
    "#             break\n",
    "\n",
    "#         count_frames+=1 \n",
    "\n",
    "#         if count_frames in frame_indices: \n",
    "#             img_sample.append(frame)\n",
    "\n",
    "#         if cv.waitKey(10) & 0xff == ord('q'): \n",
    "#             break\n",
    "        \n",
    "#     cap.release()\n",
    "#     cv.destroyAllWindows()\n",
    "\n",
    "#     #optimizing the frames \n",
    "#     resized_images = [cv.resize(img, (224, 224)) for img in img_sample]\n",
    "\n",
    "#     #saving the frames \n",
    "#     for idx,  img in enumerate(resized_images): \n",
    "\n",
    "#         if class_label : \n",
    "#             path = os.path.join('model_data/fire', video_name)\n",
    "#             os.makedirs(path, exist_ok=True)\n",
    "#             fileName= f'{idx}.jpg'\n",
    "#             cv.imwrite(os.path.join(path, fileName) , img)\n",
    "\n",
    "#         else : \n",
    "#             path = os.path.join('model_data/no_fire', video_name)\n",
    "#             os.makedirs(path , exist_ok=True)\n",
    "#             fileName = f'{idx}.jpg'\n",
    "#             Image.open(os.path.join(path, fileName) , img)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3d391ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "root_dir = '/media/ai/datasets/firesmoke_dataset'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7fe3784",
   "metadata": {},
   "outputs": [],
   "source": [
    "fire_vidoes_list  = os.listdir(os.path.join(root_dir , 'fire' ))\n",
    "no_fire_video_list  = os.listdir(os.path.join(root_dir , 'no_fire'))\n",
    "\n",
    "len(fire_vidoes_list) , len(os.listdir(os.path.join(root_dir,  'fire', 'firesmoke_2')))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5b4d9ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for vid in fire_vidoes_list: \n",
    "#     capture_vide_frames('data/videos/fire', vid, 40, 1)\n",
    "\n",
    "\n",
    "# for vid in no_fire_video_list:\n",
    "#     capture_vide_frames('data/videos/nofire' , vid , 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f841b8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import glob \n",
    "\n",
    "# fire_images = glob.glob('model_data/fire/**/*')\n",
    "# no_fire_images = glob.glob('model_data/no_fire/**/*')\n",
    "\n",
    "# print(len(fire_images), len(no_fire_images))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4acbe69",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader , random_split\n",
    "from PIL import Image\n",
    "from torchvision import transforms "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da5b9eb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FireVideoDataset(Dataset):\n",
    "    def __init__(self, root_dir, chunk_size=30):\n",
    "        self.chunk_size = chunk_size\n",
    "        self.samples = []\n",
    "        self.transform = transforms.Compose([\n",
    "            transforms.ToTensor(),  # Convert PIL image to min max scale (0-1)\n",
    "            transforms.Normalize(\n",
    "                    [0.485, 0.456, 0.406],\n",
    "                    [0.229, 0.224, 0.225]\n",
    "                    ), \n",
    "            transforms.Resize((224, 224))\n",
    "            # transforms.RandomHorizontalFlip(0.5), \n",
    "            # transforms.ColorJitter(0.2)\n",
    "        ])\n",
    "        for class_name, label in [('fire', 1), ('no_fire', 0)]:\n",
    "            class_dir = os.path.join(root_dir, class_name)\n",
    "            for chunk_folder in os.listdir(class_dir):\n",
    "                chunk_path = os.path.join(class_dir, chunk_folder)\n",
    "                \n",
    "                if os.path.isdir(chunk_path) and len(os.listdir(chunk_path)) >= chunk_size:\n",
    "                    self.samples.append((chunk_path, label))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        chunk_path, label = self.samples[idx]\n",
    "        \n",
    "        image_files = os.listdir(chunk_path)[:self.chunk_size] \n",
    "        frames = []\n",
    "\n",
    "        for fname in image_files:\n",
    "            img_path = os.path.join(chunk_path, fname)\n",
    "            img = Image.open(img_path)\n",
    "\n",
    "            if img is None:\n",
    "                continue  \n",
    "            frames.append(self.transform(img)) \n",
    "        np_frame = np.array(frames)  # its [B , C , H , W]\n",
    "        np_frame = np.transpose(np_frame, (1 , 0, 2, 3))  # becomes [C , B , H , W]\n",
    "\n",
    "        return torch.tensor(np_frame, dtype=torch.float32), torch.tensor(label, dtype=torch.float32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a13e3f71",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = FireVideoDataset(root_dir=root_dir)\n",
    "\n",
    "train_size = int(0.7 *len(dataset))\n",
    "test_size = len(dataset) - train_size\n",
    "\n",
    "train_dataset, test_dataset = random_split(dataset, [train_size, test_size])\n",
    "\n",
    "train_data_loader = DataLoader(train_dataset, batch_size=2,shuffle=True , num_workers=8)\n",
    "test_data_loader = DataLoader(test_dataset, batch_size=2, shuffle=True , num_workers=8)\n",
    "\n",
    "# for batch_idx, (data, label) in enumerate(train_data_loader):\n",
    "#     if batch_idx == 2:\n",
    "#         break\n",
    "#     print(data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2950ffe7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Subset, random_split, DataLoader\n",
    "import random\n",
    "\n",
    "# Subset 10% of the dataset\n",
    "subset_size = int(len(dataset) * 0.1)\n",
    "\n",
    "# Get a random subset of the dataset (10%)\n",
    "subset_data, _ = random_split(dataset, [subset_size, len(dataset) - subset_size])\n",
    "\n",
    "# Train/test split sizes (as integers)\n",
    "train_subset_size = int(subset_size * 0.8)\n",
    "test_subset_size = subset_size - train_subset_size\n",
    "\n",
    "# Split the subset into train and test\n",
    "subset_train, subset_test = random_split(subset_data, [train_subset_size, test_subset_size])\n",
    "\n",
    "# Create dataloaders\n",
    "check_data_loader = DataLoader(subset_train, batch_size=2, shuffle=True, num_workers=8, drop_last=True)\n",
    "test_check_data_loader = DataLoader(subset_test, batch_size=2, shuffle=True, num_workers=8, drop_last=True)\n",
    "\n",
    "# Check sizes\n",
    "print(len(check_data_loader), len(test_check_data_loader))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d316f20b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pos = 0 \n",
    "# neg = 0 \n",
    "# for i , label in check_data_loader: \n",
    "#     if 1 in label: \n",
    "#         pos += 1 \n",
    "#     else : \n",
    "#         neg += 1\n",
    "        \n",
    "# print(pos, neg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "487a62c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class FireDetector(nn.Module):\n",
    "    def __init__(self, in_channels=3, num_classes=1):\n",
    "        super().__init__()\n",
    "        self.conv_block = nn.Sequential(\n",
    "            nn.Conv3d(in_channels, 16, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm3d(16),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool3d((1, 2, 2)),  # frame , hight , width (keep fram , max pool each frame)\n",
    "\n",
    "            nn.Conv3d(16, 32, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm3d(32),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool3d((2, 2, 2)), # pick two frame , maxpool each frame \n",
    "\n",
    "            nn.Conv3d(32, 64, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm3d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.AdaptiveAvgPool3d((1, 1, 1))  # compress to (1,1,1) # pick one frame , one pixel(best one in hight and ..)\n",
    "        )\n",
    "        self.classifier = nn.Linear(64, num_classes) # [9, 64  ,  1 ,1 , 1] we resize to have channels giving to linear to pick 2 \n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv_block(x)\n",
    "        x = x.squeeze() # flatten\n",
    "        x = self.classifier(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0b88e67",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efa6c34f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fire_detector_model = FireDetector()\n",
    "# summary(fire_detector_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "863d001b",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = nn.BCEWithLogitsLoss()\n",
    "def measure_acc(ypred , ytrue): \n",
    "    print(ytrue.shape )\n",
    "    from torchmetrics import Accuracy , Recall\n",
    "    acc_fn = Accuracy(task='binary').to(device)\n",
    "    acc = acc_fn(ypred , ytrue)\n",
    "\n",
    "    sig_ypred = (torch.sigmoid(ypred) > 0.5).int()\n",
    "    recallfn = Recall(task='binary').to(device)\n",
    "    recall = recallfn(sig_ypred , ytrue)\n",
    "    return acc , recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e79673e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def train_and_eval_model(model,\n",
    "                         train_loader: DataLoader,\n",
    "                         val_loader: DataLoader,\n",
    "                         loss_fn: torch.nn.Module,\n",
    "                         number_of_epochs: int, \n",
    "                        #  pack_path_way:Optional[Callable[[int , int], torch.tensor]]\n",
    "                         ) -> Tuple[list, list, list, list]:\n",
    "\n",
    "    optimizer = torch.optim.Adam(params=model.parameters(), lr=1e-3, weight_decay=1e-3)\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    model.to(device)\n",
    "\n",
    "    train_losses, val_losses = [], []\n",
    "    train_accuracies, val_accuracies = [], []\n",
    "    train_recall , val_recalls = [] , []\n",
    "\n",
    "    for epoch in range(number_of_epochs):\n",
    "\n",
    "        # -------- TRAINING -------- #\n",
    "\n",
    "        model.train()\n",
    "        running_loss, running_acc, running_recall = 0.0, 0.0, 0.0\n",
    "        for x, y in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{number_of_epochs} - Training\"):\n",
    "            x, y = x.to(device), y.to(device)\n",
    "            y_pred = model(x)\n",
    "            y_pred = y_pred.squeeze()\n",
    "            loss = loss_fn(y_pred, y)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "            acc, recall = measure_acc(y_pred, y)\n",
    "            running_recall += recall.item()\n",
    "            running_acc += acc.item()\n",
    "        epoch_train_recall = running_recall/len(train_loader)\n",
    "        epoch_train_loss = running_loss / len(train_loader)\n",
    "        epoch_train_acc = running_acc / len(train_loader)\n",
    "        train_losses.append(epoch_train_loss)\n",
    "        train_accuracies.append(epoch_train_acc)\n",
    "        train_recall.append(epoch_train_recall)\n",
    "\n",
    "        # -------- EVALUATION -------- #\n",
    "\n",
    "        model.eval()\n",
    "        val_loss, val_acc , val_recall= 0.0, 0.0 , 0.0\n",
    "        with torch.inference_mode():\n",
    "            for x, y in tqdm(val_loader, desc=f\"Epoch {epoch+1}/{number_of_epochs} - Validation\"):\n",
    "                x, y = x.to(device), y.to(device)\n",
    "                y_pred = model(x)\n",
    "                y_pred = y_pred.squeeze()\n",
    "                loss = loss_fn(y_pred, y)\n",
    "\n",
    "                val_loss += loss.item()\n",
    "                acc, recall = measure_acc(y_pred, y)\n",
    "                val_recall += recall.item()\n",
    "                val_acc += acc.item()\n",
    "\n",
    "        epoch_val_loss = val_loss / len(val_loader)\n",
    "        epoch_val_acc = val_acc / len(val_loader)\n",
    "        epoch_val_recall = val_recall/len(val_loader)\n",
    "        val_losses.append(epoch_val_loss)\n",
    "        val_accuracies.append(epoch_val_acc)\n",
    "        val_recalls.append(epoch_val_recall)\n",
    "\n",
    "\n",
    "        print(f\"[Epoch {epoch+1}] Train Loss: {epoch_train_loss:.4f}, Train Acc: {epoch_train_acc:.4f} | Val Loss: {epoch_val_loss:.4f}, Val Acc: {epoch_val_acc:.4f}\")\n",
    "\n",
    "    return train_losses, train_accuracies , train_recall, val_losses, val_accuracies , val_recall\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d7c0dde",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy\n",
    "print(numpy.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e6826c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_loss , train_acc , recall= train_and_eval_model(fire_detector_model , train_data_loader , test_data_loader, loss_fn , 1)\n",
    "# print(f'train loss is {train_loss:2f}\\n',\n",
    "#        f'train accuracy is {train_acc:.2f}\\n', \n",
    "#        f'recall is {recall:.2f}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b37936b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_loss , test_acc , recall = eval_the_model(fire_detector_model  , test_data_loader , loss_fn)\n",
    "# print(f'train loss is {train_loss:2f}\\n',\n",
    "#        f'train accuracy is {train_acc:.2f}\\n', \n",
    "#        f'recall is {recall:.2f}\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3674a598",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from torchvision.models.video import r3d_18 , R3D_18_Weights\n",
    "# resnet_model = r3d_18(weights=R3D_18_Weights.DEFAULT)\n",
    "# resnet_model.fc = nn.Linear(resnet_model.fc.in_features, 1)  # Binary classification\n",
    "\n",
    "# resnet_model.fc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "827b7b69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_loss , train_acc , recall= train_and_eval_model(resnet_model , 1, train_data_loader , loss_fn  )\n",
    "# print(f'train loss is {train_loss:2f}\\n',\n",
    "#        f'train accuracy is {train_acc:.2f}\\n', \n",
    "#        f'recall is {recall:.2f}\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eeade5ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_loss , test_acc , recall = eval_the_model(resnet_model ,  test_data_loader, loss_fn)\n",
    "# print(f'test loss is {test_loss}\\n',\n",
    "#        f'test accuracy is {test_acc}\\n', \n",
    "#        f'recall is {recall}\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaa8ffec",
   "metadata": {},
   "source": [
    "### trying conv2dand1D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8891a98b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FireDetectorConv2D1D(nn.Module): \n",
    "    def __init__(self, in_channels = 3 , num_classes=1): \n",
    "        super().__init__() \n",
    "        self.spatial_extractor = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, 32, kernel_size = 3, padding=1),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(), \n",
    "            nn.MaxPool2d(2),\n",
    "\n",
    "            nn.Conv2d(32 , 64, kernel_size = 3 , padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.AdaptiveAvgPool2d(1),\n",
    "        )\n",
    "\n",
    "        self.temporal_extractor = nn.Sequential(\n",
    "            nn.Conv1d(64, 128 , 3 ,padding = 1 ),\n",
    "            nn.BatchNorm1d(128), \n",
    "            nn.ReLU(), \n",
    "            nn.AdaptiveAvgPool1d(1) \n",
    "        )\n",
    "\n",
    "        self.clasifier = nn.Linear(128, num_classes)\n",
    "\n",
    "    def forward(self , x): \n",
    "            B , C , T , H , W = x.size()\n",
    "            x = x.permute(0, 2, 1, 3 ,4)  # (B, T, C, H, W)\n",
    "            x = x.reshape(B * T , C , H , W)\n",
    "            x = self.spatial_extractor(x) # (B*T, 64, 1, 1)\n",
    "            x = x.view(B , T , 64)  # (B, T, 64)\n",
    "\n",
    "            x = x.permute(0 , 2 , 1) # (B , 64, T)\n",
    "            x = self.temporal_extractor(x) # (B , 128, 1)\n",
    "            x = x.squeeze(-1) # (B , 128)\n",
    "\n",
    "            return self.clasifier(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f1132c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "model2d_plus = FireDetectorConv2D1D()\n",
    "summary(model2d_plus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c6160bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_losses, train_accuracies , train_recall, val_losses, val_accuracies , val_recalls = train_and_eval_model(FireDetectorConv2D1D(), check_data_loader, test_check_data_loader, loss_fn, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0983a163",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\n",
    "       f'train loss is {train_losses[-1]:2f}\\n',\n",
    "       f'train accuracy is {train_accuracies[-1]:.2f}\\n', \n",
    "       f'recall is {train_recall[-1]:.2f}\\n'\n",
    "       f'val loss is {val_losses[-1]:2f}\\n',\n",
    "       f'val accuracy is {val_accuracies[-1]:.2f}\\n', \n",
    "       # f'val recall is {val_recalls[-1]:.2f}\\n'\n",
    "       )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf85b208",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_loss , test_acc , recall = eval_the_model(model2d_plus, test_data_loader, loss_fn )\n",
    "# print(f'test loss is {test_loss:2f}\\n',\n",
    "#        f'test accuracy is {test_acc:.2f}\\n', \n",
    "#        f'recall is {recall:.2f}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a560f6e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FireDetectorMain3D(nn.Module): \n",
    "    def __init__(self, in_channels , out_channels, kernel_size, padding): \n",
    "        super().__init__()\n",
    "        T, H, W = kernel_size\n",
    "\n",
    "        self.spatial_conv = nn.Conv3d(\n",
    "            in_channels,  out_channels , kernel_size=(1 , H, W), padding=(0, padding , padding),\n",
    "        )\n",
    "\n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "        self.temporal_conv = nn.Conv3d(\n",
    "            out_channels , out_channels , kernel_size=(T , 1, 1) , padding=(padding, 0 , 0)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x): \n",
    "        x = self.spatial_conv(x)\n",
    "        x = self.relu(x) \n",
    "        x = self.temporal_conv(x) \n",
    "        return x \n",
    "\n",
    "class Residual(nn.Module): \n",
    "    def __init__(self, channels , kernel_size, padding =1):\n",
    "        super().__init__()\n",
    "        self.conv1 = FireDetectorMain3D(channels, channels, kernel_size , padding)\n",
    "        self.norm1 = nn.LayerNorm(channels)\n",
    "        self.conv2 = FireDetectorMain3D(channels ,channels, kernel_size , padding)\n",
    "        self.norm2 = nn.LayerNorm(channels)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x) : \n",
    "        Residual = x \n",
    "        out = self.conv1(x) #(N, C, D, H, W)\n",
    "        out = out.permute(0, 2 , 3 , 4 , 1)\n",
    "        out = self.norm1(out)\n",
    "        out= out.permute(0 , 4 , 1 , 2, 3)\n",
    "        # out = self.relu(out)\n",
    "\n",
    "        out = self.conv2(out)\n",
    "        out = out.permute(0, 2 , 3 , 4 , 1)\n",
    "        out = self.norm2(out)\n",
    "        out= out.permute(0 , 4 , 1 , 2, 3)\n",
    "\n",
    "        out += Residual\n",
    "        out = self.relu(out)\n",
    "        return out \n",
    "    \n",
    "class FireDetectorWithResidual(nn.Module): \n",
    "        def __init__(self , in_channels =3 , num_clases =1) :\n",
    "            super().__init__()\n",
    "            self.initial_conv = FireDetectorMain3D(in_channels, 16 , kernel_size=(3, 7, 7) , padding=1)\n",
    "            self.bn = nn.BatchNorm3d(16)\n",
    "            self.relu = nn.ReLU() \n",
    "            self.pool1 = nn.MaxPool3d((1 , 2 ,2))\n",
    "            self.res_block1 = Residual(16, kernel_size=(3 ,3 ,3))\n",
    "            self.pool2 = nn.MaxPool3d((2 ,2 ,2))\n",
    "\n",
    "            self.res_block2 = Residual(16, kernel_size=(3 ,3 ,3))\n",
    "            self.adaptive_pool = nn.AdaptiveAvgPool3d((1 ,1,1))\n",
    "\n",
    "            self.classifier = nn.Linear(16 , num_clases)\n",
    "\n",
    "        def forward(self  , x): \n",
    "            x = self.initial_conv(x)\n",
    "            x = self.bn(x)\n",
    "            x = self.relu(x)\n",
    "            x = self.pool1(x)\n",
    "\n",
    "            x = self.res_block1(x)\n",
    "            x = self.pool2(x)\n",
    "\n",
    "            x = self.res_block2(x)\n",
    "            x = self.adaptive_pool(x)\n",
    "\n",
    "            x = x.flatten(1)\n",
    "            x = self.classifier(x)\n",
    "\n",
    "            return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9db1697",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_sample = next(iter(train_data_loader))[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4adc50d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "resnet2d1d = FireDetectorWithResidual()\n",
    "# summary(resnet2d1d, input_size=test_sample.shape)  # (batch, C, D, H, W)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fd1b5bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_losses, train_accuracies , train_recall, val_losses, val_accuracies , val_recalls  = train_and_eval_model(resnet2d1d, check_data_loader, test_check_data_loader, loss_fn, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b85333d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\n",
    "       f'train loss is {train_losses[-1]:2f}\\n',\n",
    "       f'train accuracy is {train_accuracies[-1]:.2f}\\n', \n",
    "       f'recall is {train_recall[-1]:.2f}\\n'\n",
    "       f'val loss is {val_losses[-1]:2f}\\n',\n",
    "       f'val accuracy is {val_accuracies[-1]:.2f}\\n', \n",
    "       # f'val recall is {val_recalls[-1]:.2f}\\n'\n",
    "       )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a25708ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_loss, test_acc , test_recall = eval_the_model(resnet2d1d , test_data_loader, loss_fn)\n",
    "# print(f'test loss is {test_loss}\\n',\n",
    "#        f'test accuracy is {test_acc}\\n', \n",
    "#        f'recall is {recall}\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32ef4c45",
   "metadata": {},
   "source": [
    "## slow fast model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "771ec5cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "# Choose the `slowfast_r50` model \n",
    "slow_fast_model = torch.hub.load('facebookresearch/pytorchvideo', 'slowfast_r50' , pretrained=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c76a06e",
   "metadata": {},
   "outputs": [],
   "source": [
    "summary(slow_fast_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "214f1106",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# from torchvision.transforms import Compose, Lambda, CenterCrop, Normalize \n",
    "\n",
    "# from torchvision.transforms.v2 import (\n",
    "#     CenterCropVideo,\n",
    "#     NormalizeVideo,\n",
    "# )\n",
    "from torchvision.transforms import v2 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27c89ac2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytorchvideo.transforms import (\n",
    "    ApplyTransformToKey,\n",
    "    ShortSideScale,\n",
    ")\n",
    "from torchvision.transforms import Compose\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98ed33ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "slow_fast_model = slow_fast_model.eval()\n",
    "slow_fast_model = slow_fast_model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe2fbd72",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.transforms import Compose, Lambda, CenterCrop, Normalize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e684c49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# video = EncodedVideo.from_path('data/videos/fire/fire_2.mp4')\n",
    "# clip = video.get_clip(start_sec=0, end_sec=1.0)  # Extract a 2-second segment\n",
    "# clip['video'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9a95868",
   "metadata": {},
   "outputs": [],
   "source": [
    "# transform = ApplyTransformToKey(\n",
    "#     key=\"video\",\n",
    "#     transform=Compose([\n",
    "#         # UniformTemporalSubsample(32), # its for videos , samples are ready\n",
    "#         Lambda(lambda x: x / 255.0),\n",
    "#         NormalizeVideo(mean=[0.45, 0.45, 0.45], std=[0.225, 0.225, 0.225]),\n",
    "#         ShortSideScale(256),\n",
    "#         CenterCropVideo(224),\n",
    "#     ])\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "289bca24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# clip['video'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b5af7d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# myclip = {'video' :  clip['video']}\n",
    "# myclip['video'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b60227da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ts_image = transform(myclip)\n",
    "# final_ts_image = ts_image['video'].unsqueeze(0)\n",
    "# final_ts_image.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a405a460",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pack_pathway_output(frames, alpha=4):\n",
    "    fast_pathway = frames\n",
    "    slow_pathway = frames[:, :, ::alpha, :, :]  # temporal subsampling\n",
    "    return [slow_pathway, fast_pathway]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9782d09",
   "metadata": {},
   "outputs": [],
   "source": [
    "test = cv.imread('image.png')\n",
    "test = test.reshape(1, 1, 768, 768, 3)\n",
    "test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "503b1f37",
   "metadata": {},
   "outputs": [],
   "source": [
    "test = test.transpose(0, 4 , 1, 2, 3)\n",
    "test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b80c6ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "# Example image (H, W, C)\n",
    "img = np.random.rand(768, 768, 3).astype(np.float32)\n",
    "\n",
    "# Convert to torch tensor and permute to (C, H, W)\n",
    "img_tensor = torch.from_numpy(img).permute(2, 0, 1)  # Now (3, 768, 768)\n",
    "\n",
    "# Add time and batch dimensions → (1, 3, 1, 768, 768)\n",
    "img_tensor_5d = img_tensor.unsqueeze(0).unsqueeze(2)\n",
    "\n",
    "print(img_tensor_5d.shape)  # Output: torch.Size([1, 3, 1, 768, 768])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9decbd51",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "img = np.random.rand(3, 768, 768)  # (C, H, W)\n",
    "\n",
    "# Repeat frame 20 times along depth (time) axis\n",
    "depth = 32\n",
    "video = np.repeat(img[np.newaxis, ...], depth, axis=0)  # Shape: (D, C, H, W)\n",
    "\n",
    "# Rearrange to (C, D, H, W)\n",
    "video = np.transpose(video, (1, 0, 2, 3))  # Shape: (C, D, H, W)\n",
    "\n",
    "# Add batch dimension: (1, C, D, H, W)\n",
    "video = np.expand_dims(video, axis=0)\n",
    "\n",
    "print(video.shape)  # Output: (1, 3, 20, 768, 768)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c4e2a58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# video = torch.from_numpy(video)\n",
    "# video.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14e42c01",
   "metadata": {},
   "outputs": [],
   "source": [
    "video = video.float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11be85ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "video = video.squeeze()\n",
    "video.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a4f0339",
   "metadata": {},
   "outputs": [],
   "source": [
    "ob_img = {'video' : video}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7578977",
   "metadata": {},
   "outputs": [],
   "source": [
    "ob_img['video'].shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e378091",
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.Size([2, 3, 30, 224, 224])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "967beb50",
   "metadata": {},
   "outputs": [],
   "source": [
    "ts_img = transform(ob_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7afe1c8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "ts_img['video'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f62978b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_ts_img = ts_img['video'].unsqueeze(0)\n",
    "final_ts_img.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e6aa194",
   "metadata": {},
   "outputs": [],
   "source": [
    "ready = pack_pathway_output(final_ts_img)\n",
    "ready[0].shape , ready[1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "defe10f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.inference_mode(): \n",
    "    slow_fast_model.to('cpu') \n",
    "    output = slow_fast_model(ready)\n",
    "    output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "458c203c",
   "metadata": {},
   "outputs": [],
   "source": [
    "output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e623d517",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.argmax(output , dim=1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40448c7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_class = torch.softmax(output, dim=1)\n",
    "predicted_class.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "384cf9e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.argmax(predicted_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce0c998e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def slow_fast_model_train(model, train_loader:DataLoader , val_loader:DataLoader, \n",
    "                          loss_fn,\n",
    "                          number_of_epochs:int ,\n",
    "                          pathway_alpha:int)-> Tuple[list , list , list , list]:\n",
    "\n",
    "    def pack_pathway_output(frames, alpha):\n",
    "        fast_pathway = frames\n",
    "        slow_pathway = frames[:, :, ::alpha, :, :]  # temporal subsampling\n",
    "        return [slow_pathway, fast_pathway]\n",
    "    \n",
    "    def padd_to_32(data:torch.tensor) : \n",
    "            T = data.shape[2]\n",
    "            if T == 32 : \n",
    "                return data \n",
    "            elif T < 32: \n",
    "                padd = 32 - T \n",
    "                st1 =  data[:,:,-1:,:,:].repeat(1, 1, padd , 1, 1)\n",
    "                stacked = torch.cat([st1, data] , dim=2)\n",
    "                return stacked\n",
    "            else: \n",
    "                return data[:, :, :32, : , :]\n",
    "            \n",
    "\n",
    "    def transform_data(data:torch.tensor): \n",
    "\n",
    "        # transform = ApplyTransformToKey(\n",
    "        # key=\"video\",\n",
    "        # transform=Compose([\n",
    "        #     # UniformTemporalSubsample(32), # its for videos , samples are ready\n",
    "        #     Lambda(lambda x: x / 255.0),\n",
    "        #     NormalizeVideo(mean=[0.45, 0.45, 0.45], std=[0.225, 0.225, 0.225]),\n",
    "        #     ShortSideScale(256),\n",
    "        #     CenterCropVideo(224),\n",
    "        # ]))\n",
    "        \n",
    "        #input shape N, C , D , H , W \n",
    "        # p_data = data.permute(0, 2 , 1, 3 ,4)\n",
    "        # N, D, C, H, W = p_data.shape\n",
    "        # r_data = p_data.reshape(N * D, C , H, W)\n",
    "        # r_data = {'video' : r_data}\n",
    "        # t_data = transform(r_data)['video']\n",
    "        # data = t_data.reshape(N, D , C , H , W)    \n",
    "        # data = data.permute(0 , 2 , 1, 3, 4)  \n",
    "\n",
    "        data = pack_pathway_output(data , pathway_alpha)   # this transofrms frame to slow and fast double frames \n",
    "        \n",
    "        return data\n",
    "        \n",
    "        \n",
    "        \n",
    "        ## in progrsss\n",
    "\n",
    "    optimizer = torch.optim.Adam(params=model.parameters(), lr=1e-3, weight_decay=1e-3)\n",
    "\n",
    "            \n",
    "    optimizer = torch.optim.Adam(params=model.parameters(), lr=1e-3)\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    model.to(device)\n",
    "\n",
    "    train_losses, val_losses = [], []\n",
    "    train_accuracies, val_accuracies = [], []\n",
    "    train_recalls , val_recalls = [] , []\n",
    "\n",
    "    for epoch in range(number_of_epochs):\n",
    "        running_loss , running_acc , running_recall = 0 , 0 , 0 \n",
    "        for x , y in tqdm(train_loader , desc=f'Epoch {epoch+1}/{number_of_epochs} - Training' ): \n",
    "            # ----- TRAINING ----- #\n",
    "            x , y = x.to(device) , y.to(device)\n",
    "            x = padd_to_32(x)\n",
    "            x = transform_data(x)\n",
    "\n",
    "            model.train() \n",
    "            ypred = model(x).squeeze()\n",
    "            ypred = torch.argmax(ypred ,  dim=1)\n",
    "            print(ypred.shape)\n",
    "            loss = loss_fn(ypred, y)\n",
    "            optimizer.zero_grad() \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            acc , recall = measure_acc(ypred, y)\n",
    "            running_loss += loss.item() \n",
    "            running_acc  += acc \n",
    "            running_recall += recall\n",
    "        epoch_train_recall = running_recall/len(train_loader)\n",
    "        epoch_train_loss = running_loss / len(train_loader)\n",
    "        epoch_train_acc = running_acc / len(train_loader)\n",
    "        train_losses.append(epoch_train_loss)\n",
    "        train_accuracies.append(epoch_train_acc)\n",
    "        train_recalls.append(epoch_train_recall)        \n",
    "\n",
    "    # ----- EVALUATION ---- # \n",
    "\n",
    "        model.eval()\n",
    "        val_loss, val_acc , val_recall= 0.0, 0.0 , 0.0\n",
    "        with torch.inference_mode():\n",
    "            for x, y in tqdm(val_loader, desc=f\"Epoch {epoch+1}/{number_of_epochs} - Validation\"):\n",
    "                x, y = x.to(device), y.to(device)\n",
    "                y_pred = model(x)\n",
    "                y_pred = y_pred.squeeze()\n",
    "                loss = loss_fn(y_pred, y)\n",
    "\n",
    "                val_loss += loss.item()\n",
    "                acc, recall = measure_acc(y_pred, y)\n",
    "                val_recall += recall.item()\n",
    "                val_acc += acc.item()\n",
    "\n",
    "        epoch_val_loss = val_loss / len(val_loader)\n",
    "        epoch_val_acc = val_acc / len(val_loader)\n",
    "        epoch_val_recall = val_recall/len(val_loader)\n",
    "        val_losses.append(epoch_val_loss)\n",
    "        val_accuracies.append(epoch_val_acc)\n",
    "        val_recalls.append(epoch_val_recall)\n",
    "\n",
    "        print(f\"[Epoch {epoch+1}] Train Loss: {epoch_train_loss:.4f}, Train Acc: {epoch_train_acc:.4f} | Val Loss: {epoch_val_loss:.4f}, Val Acc: {epoch_val_acc:.4f}\")\n",
    "\n",
    "    return train_losses, train_accuracies , train_recall, val_losses, val_accuracies , val_recall\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e86dca3",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_losses, train_accuracies ,train_recall, val_losses,val_accuracies , val_recall  = slow_fast_model_train(slow_fast_model , check_data_loader, test_check_data_loader , loss_fn, 1  , 4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "135fcde3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1338f435",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
