{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "879440e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch \n",
    "import cv2 as cv\n",
    "import os \n",
    "import matplotlib.pyplot as plt\n",
    "from torchinfo import summary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64eee125",
   "metadata": {},
   "outputs": [
    {
     "ename": "_IncompleteInputError",
     "evalue": "incomplete input (371091138.py, line 15)",
     "output_type": "error",
     "traceback": [
      "  \u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 15\u001b[39m\n\u001b[31m    \u001b[39m\n    ^\n\u001b[31m_IncompleteInputError\u001b[39m\u001b[31m:\u001b[39m incomplete input\n"
     ]
    }
   ],
   "source": [
    "from typing import Literal\n",
    "\n",
    "def capture_video_frame(video:str, videoName:str, number_of_frames:int , class_label:Literal[0,1]): \n",
    "    vid_path = os.path.join(video , videoName)\n",
    "    cap = capture_video_frame(vid_path)\n",
    "    total_frames = cv.get(cv.CAP_PROP_FRAME_COUNT)\n",
    "    frame_indices=  np.random.uniform(1, total_frames, number_of_frames)\n",
    "    img_samples = []\n",
    "    frame_count = 0 \n",
    "    while cap : \n",
    "        is_playing , frame = cap.read()\n",
    "        if not is_playing : \n",
    "            break \n",
    "        frame_count +=1 \n",
    "\n",
    "        if frame_count in frame_indices: \n",
    "            img_samples.append(frame)\n",
    "\n",
    "        if cv.waitKey(10) & 0xff == ord('q'): \n",
    "            break\n",
    "    cap.relase()\n",
    "    cv.destroyAllWindows()\n",
    "\n",
    "    resized_image = [cv.resize(img , 224, 224) for img in img_samples]\n",
    "    for idx , img in enumerate(resized_image): \n",
    "        if class_label: \n",
    "            path = os.path.join('model_data/fire', videoName)\n",
    "            os.makedirs(path, exist_ok=True)\n",
    "            file_name = f'{img}.jpg'\n",
    "            cv.imwrite(os.path.join(path, file_name), img)\n",
    "        else: \n",
    "            \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d3650a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class Conv2Plus1D(nn.Module):\n",
    "\n",
    "    def __init__(self, in_channels, out_channels, kernel_size, padding):\n",
    "        super().__init__()\n",
    "        # kernel_size is a tuple like (T, H, W)\n",
    "        T, H, W = kernel_size\n",
    "        \n",
    "        self.spatial_conv = nn.Conv3d(\n",
    "            in_channels, out_channels, kernel_size=(1, H, W), padding=(0, padding, padding)\n",
    "        )\n",
    "        self.temporal_conv = nn.Conv3d(\n",
    "            out_channels, out_channels, kernel_size=(T, 1, 1), padding=(padding, 0, 0)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.spatial_conv(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.temporal_conv(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class ResidualMain(nn.Module):\n",
    "\n",
    "    def __init__(self, channels, kernel_size, padding=1):\n",
    "        super().__init__()\n",
    "        self.conv1 = Conv2Plus1D(channels, channels, kernel_size, padding)\n",
    "        self.norm1 = nn.LayerNorm([channels, 1, 1, 1])  # LayerNorm over C, D, H, W\n",
    "        self.conv2 = Conv2Plus1D(channels, channels, kernel_size, padding)\n",
    "        self.norm2 = nn.LayerNorm([channels, 1, 1, 1])\n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "        out = self.conv1(x)\n",
    "        # LayerNorm expects (N, C, D, H, W), but works on last dims. Need permute:\n",
    "        # We'll permute to (N, D, H, W, C), apply LayerNorm over last dim, then permute back\n",
    "        out = out.permute(0, 2, 3, 4, 1)  # N, D, H, W, C\n",
    "        out = self.norm1(out)\n",
    "        out = out.permute(0, 4, 1, 2, 3)  # N, C, D, H, W\n",
    "        \n",
    "        out = self.relu(out)\n",
    "        out = self.conv2(out)\n",
    "        \n",
    "        out = out.permute(0, 2, 3, 4, 1)\n",
    "        out = self.norm2(out)\n",
    "        out = out.permute(0, 4, 1, 2, 3)\n",
    "        \n",
    "        out += residual\n",
    "        out = self.relu(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "# Example usage similar to your FireDetector conv_block\n",
    "\n",
    "class FireDetectorWithResidual(nn.Module):\n",
    "    def __init__(self, in_channels=3, num_classes=1):\n",
    "        super().__init__()\n",
    "        self.initial_conv = Conv2Plus1D(in_channels, 16, kernel_size=(3,7,7), padding=1)\n",
    "        self.bn = nn.BatchNorm3d(16)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.pool1 = nn.MaxPool3d((1, 2, 2))\n",
    "        \n",
    "        self.res_block1 = ResidualMain(16, kernel_size=(3,3,3))\n",
    "        self.pool2 = nn.MaxPool3d((2, 2, 2))\n",
    "        \n",
    "        self.res_block2 = ResidualMain(16, kernel_size=(3,3,3))\n",
    "        self.adaptive_pool = nn.AdaptiveAvgPool3d((1, 1, 1))\n",
    "        \n",
    "        self.classifier = nn.Linear(16, num_classes)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.initial_conv(x)\n",
    "        x = self.bn(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.pool1(x)\n",
    "        \n",
    "        x = self.res_block1(x)\n",
    "        x = self.pool2(x)\n",
    "        \n",
    "        x = self.res_block2(x)\n",
    "        x = self.adaptive_pool(x)\n",
    "        \n",
    "        x = x.flatten(1)  # flatten all except batch dim\n",
    "        x = self.classifier(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42d6c355",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch \n",
    "import cv2 as cv\n",
    "import os \n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import Dataset, DataLoader , random_split\n",
    "from PIL import Image\n",
    "from torchvision import transforms "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2aa5679",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\alchemist\\Desktop\\computer_vision\\venv\\Lib\\site-packages\\torchvision\\transforms\\_functional_video.py:6: UserWarning: The 'torchvision.transforms._functional_video' module is deprecated since 0.12 and will be removed in the future. Please use the 'torchvision.transforms.functional' module instead.\n",
      "  warnings.warn(\n",
      "c:\\Users\\alchemist\\Desktop\\computer_vision\\venv\\Lib\\site-packages\\torchvision\\transforms\\_transforms_video.py:22: UserWarning: The 'torchvision.transforms._transforms_video' module is deprecated since 0.12 and will be removed in the future. Please use the 'torchvision.transforms' module instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from typing import Dict\n",
    "import json\n",
    "import urllib\n",
    "from torchvision.transforms import Compose, Lambda\n",
    "from torchvision.transforms._transforms_video import (\n",
    "    CenterCropVideo,\n",
    "    NormalizeVideo,\n",
    ")\n",
    "from pytorchvideo.data.encoded_video import EncodedVideo\n",
    "# from pytorchvideo.transforms import (\n",
    "#     ApplyTransformToKey,\n",
    "#     ShortSideScale,\n",
    "#     UniformTemporalSubsample,\n",
    "#     UniformCropVideo\n",
    "# ) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2dfd7d42",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict\n",
    "import json\n",
    "import urllib\n",
    "\n",
    "from torchvision.transforms import Compose, Lambda, CenterCrop, Normalize\n",
    "from pytorchvideo.data.encoded_video import EncodedVideo\n",
    "# Optional: if needed\n",
    "# from pytorchvideo.transforms import (\n",
    "#     ApplyTransformToKey,\n",
    "#     ShortSideScale,\n",
    "#     UniformTemporalSubsample,\n",
    "#     UniformCropVideo\n",
    "# )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b53e7634",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 30, 720, 1280])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "video = EncodedVideo.from_path('data/videos/fire/fire_2.mp4')\n",
    "clip = video.get_clip(start_sec=0, end_sec=1.0)  # Extract a 2-second segment\n",
    "clip['video'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "05c04cad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 30, 720, 1280])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "video = EncodedVideo.from_path('data/videos/fire/fire_3.mp4')\n",
    "clip = video.get_clip(start_sec=0, end_sec=1.0)  # Extract a 2-second segment\n",
    "clip['video'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83a8d84a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Load video & extract clip\n",
    "video = EncodedVideo.from_path('data/videos/fire/fire_2.mp4')\n",
    "clip = video.get_clip(start_sec=0, end_sec=1.0)\n",
    "\n",
    "# 2. Transform pipeline\n",
    "transform = ApplyTransformToKey(\n",
    "    key=\"video\",\n",
    "    transform=Compose([\n",
    "        UniformTemporalSubsample(30),\n",
    "        Lambda(lambda x: x / 255.0),\n",
    "        NormalizeVideo(mean=[0.45] * 3, std=[0.225] * 3),\n",
    "        ShortSideScale(256),\n",
    "        CenterCropVideo(224),\n",
    "    ])\n",
    ")\n",
    "\n",
    "# 3. Apply transform\n",
    "video_tensor = {'video': clip['video']}\n",
    "ts_image = transform(video_tensor)\n",
    "final_ts_image = ts_image['video'].unsqueeze(0)  # [1, 3, 30, 224, 224]\n",
    "\n",
    "# 4. Pack into SlowFast input\n",
    "def pack_pathway_output(frames, alpha=4):\n",
    "    fast_pathway = frames\n",
    "    slow_pathway = frames[:, :, ::alpha, :, :]\n",
    "    return [slow_pathway, fast_pathway]\n",
    "\n",
    "inputs = pack_pathway_output(final_ts_image)\n",
    "\n",
    "# 5. Load and run model\n",
    "import torch\n",
    "slow_fast_model = torch.hub.load('facebookresearch/pytorchvideo', 'slowfast_r50', pretrained=True)\n",
    "slow_fast_model.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    preds = slow_fast_model(inputs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cf8f6380",
   "metadata": {},
   "outputs": [],
   "source": [
    "# side_size = 256\n",
    "# mean = [0.45, 0.45, 0.45]\n",
    "# std = [0.225, 0.225, 0.225]\n",
    "# crop_size = 256\n",
    "# num_frames = 32\n",
    "# sampling_rate = 2\n",
    "# frames_per_second = 30\n",
    "# slowfast_alpha = 4\n",
    "# num_clips = 10\n",
    "# num_crops = 3\n",
    "\n",
    "# class PackPathway(torch.nn.Module):\n",
    "#     \"\"\"\n",
    "#     Transform for converting video frames as a list of tensors. \n",
    "#     \"\"\"\n",
    "#     def __init__(self):\n",
    "#         super().__init__()\n",
    "        \n",
    "#     def forward(self, frames: torch.Tensor):\n",
    "#         fast_pathway = frames\n",
    "#         # Perform temporal sampling from the fast pathway.\n",
    "#         slow_pathway = torch.index_select(\n",
    "#             frames,\n",
    "#             1,\n",
    "#             torch.linspace(\n",
    "#                 0, frames.shape[1] - 1, frames.shape[1] // slowfast_alpha\n",
    "#             ).long(),\n",
    "#         )\n",
    "#         frame_list = [slow_pathway, fast_pathway]\n",
    "#         return frame_list\n",
    "\n",
    "# transform =  ApplyTransformToKey(\n",
    "#     key=\"video\",\n",
    "#     transform=Compose(\n",
    "#         [\n",
    "#             UniformTemporalSubsample(num_frames),\n",
    "#             Lambda(lambda x: x/255.0),\n",
    "#             NormalizeVideo(mean, std),\n",
    "#             ShortSideScale(\n",
    "#                 size=side_size\n",
    "#             ),\n",
    "#             CenterCropVideo(crop_size),\n",
    "#             PackPathway()\n",
    "#         ]\n",
    "#     ),\n",
    "# )\n",
    "\n",
    "# # The duration of the input clip is also specific to the model.\n",
    "# clip_duration = (num_frames * sampling_rate)/frames_per_second"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfc12db0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from typing import Tuple\n",
    "# from tqdm import tqdm \n",
    "\n",
    "# def train_the_model(model , number_of_epochs:int ,\n",
    "#                      data_loader:DataLoader, loss_fn:torch.nn.Module , \n",
    "#                      ) -> Tuple[float, float] :\n",
    "    \n",
    "#     optimizer = torch.optim.Adam(params=model.parameters() , lr=1e-1, weight_decay=1e-3)\n",
    "    \n",
    "#     device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "#     model.to(device)\n",
    "    \n",
    "#     for epoch in range(number_of_epochs): \n",
    "#         model.train()\n",
    "#         batch_loss = 0\n",
    "#         batch_acc = 0 \n",
    "#         batch_recall = 0\n",
    "\n",
    "#         for x , y in tqdm(data_loader) : \n",
    "#             x = x.to(device)\n",
    "#             y = y.to(device)\n",
    "#             ypred = model(x)\n",
    "#             # ypred = ypred.squeeze()\n",
    "#             loss = loss_fn(ypred, y)\n",
    "            \n",
    "#             batch_loss += loss.item() \n",
    "#             acc , recall = measure_acc(ypred, y) \n",
    "#             batch_acc += acc.item()\n",
    "#             batch_recall += recall.item()\n",
    "#             loss.backward()\n",
    "#             optimizer.step()\n",
    "#         batch_loss /= len(data_loader)\n",
    "#         batch_acc /= len(data_loader)\n",
    "#         batch_recall /= len(data_loader)\n",
    "\n",
    "        \n",
    "#     return batch_loss , batch_acc, batch_recall\n",
    "\n",
    "\n",
    "# def eval_the_model(model:torch.nn.Module ,data_loader:DataLoader, loss_fn:torch.nn.Module ):\n",
    "#     test_loss = 0 \n",
    "#     test_acc = 0 \n",
    "#     batch_recall = 0\n",
    "\n",
    "#     model.eval()\n",
    "#     with torch.inference_mode(): \n",
    "#         for x , y in data_loader: \n",
    "#             x = x.to(device)\n",
    "#             y = y.to(device)\n",
    "#             ypred = model(x)\n",
    "#             ypred = np.squeeze(ypred)\n",
    "#             loss = loss_fn(ypred , y)\n",
    "#             test_loss+=loss.item()\n",
    "#             acc , recall = measure_acc(ypred , y)\n",
    "#             test_acc += acc.item()\n",
    "#             batch_recall += recall.item()\n",
    "#         test_loss /= len(data_loader)\n",
    "#         test_acc /= len(data_loader)\n",
    "#         batch_recall /= len(data_loader)\n",
    "    \n",
    "#     return test_loss, test_acc , batch_recall\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36604eee",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Subset, DataLoader\n",
    "import random\n",
    "from collections import defaultdict\n",
    "\n",
    "def get_balanced_subset(dataset, samples_per_class):\n",
    "    class_indices = defaultdict(list)\n",
    "\n",
    "    # Group indices by class label\n",
    "    for idx, (_, label) in enumerate(dataset):\n",
    "        class_indices[label].append(idx)\n",
    "\n",
    "    # Sample 'samples_per_class' items from each class\n",
    "    balanced_indices = []\n",
    "    for label, indices in class_indices.items():\n",
    "        # Only sample up to available samples\n",
    "        sampled = random.sample(indices, min(samples_per_class, len(indices)))\n",
    "        balanced_indices.extend(sampled)\n",
    "\n",
    "    # Create a subset\n",
    "    return Subset(dataset, balanced_indices)\n",
    "\n",
    "# Example usage:\n",
    "balanced_subset = get_balanced_subset(dataset, samples_per_class=10)\n",
    "\n",
    "check_data_loader = DataLoader(balanced_subset, batch_size=2, shuffle=True, num_workers=8, drop_last=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "aadfe657",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "defaultdict(<class 'list'>, {'cat': [1, 2], 'dog': [3]})\n"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "d = defaultdict(list)\n",
    "d['cat'].append(1)\n",
    "d['cat'].append(2)\n",
    "d['dog'].append(3)\n",
    "\n",
    "print(d)  # {'cat': [1, 2], 'dog': [3]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "dd413daf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7f2322ad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 3, 30, 224, 224])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = torch.rand((2, 3, 30, 224, 224))\n",
    "test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "43f2dc12",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 3, 2, 224, 224])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "padding = test[:, :, -1:, :, :].repeat(1, 1, 2, 1, 1)\n",
    "padding.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "238fa0b8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 3, 32, 224, 224])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_test = torch.cat([padding , test] , dim=2)\n",
    "new_test.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "201b20fb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 3, 60, 224, 224])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.repeat(test, 2, axis=2).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "7e9fa84a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 3, 2, 224, 224])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test[:,:,-1:,:,:].repeat(1, 1, 2, 1, 1).shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "3a6c2f23",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([4])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "array = np.array([1, 2, 3,4])\n",
    "array[-1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "e11abc5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def padd_to_32(data:torch.tensor) : \n",
    "        T = data.size(2)\n",
    "        if T == 32 : \n",
    "            return data \n",
    "        elif T < 32: \n",
    "            padd = 32 - T \n",
    "            st1 =  data[:,:,-1:,:,:].repeat(1, 1, padd , 1, 1)\n",
    "            stacked = torch.cat([st1, data] , dim=2)\n",
    "            return stacked\n",
    "        else: \n",
    "            return data[:, :, :32, : , :]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "ca7d07dd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 3, 45, 224, 224])"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = torch.rand((2, 3, 45, 224, 224))\n",
    "test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "b524dbfc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 3, 32, 224, 224])"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "padd_to_32(test).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "7c35d580",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # train.py\n",
    "# import argparse\n",
    "\n",
    "# def get_default_config():\n",
    "#     return {\n",
    "#         \"learning_rate\": 0.001,\n",
    "#         \"epochs\": 10,\n",
    "#         \"batch_size\": 32,\n",
    "#         \"dataset_path\": \"data/fire_videos\",\n",
    "#         \"save_dir\": \"checkpoints/\"\n",
    "#     }\n",
    "\n",
    "# def parse_args(): \n",
    "#     default_cfg = get_default_config()\n",
    "\n",
    "#     parser = argparse.ArgumentParser(description=\"Fire detection trainer\")\n",
    "\n",
    "#     # Add arguments with default values from config\n",
    "#     parser.add_argument('--learning_rate', type=float, default=default_cfg['learning_rate'])\n",
    "#     parser.add_argument('--epochs', type=int, default=default_cfg['epochs'])\n",
    "#     parser.add_argument('--batch_size', type=int, default=default_cfg['batch_size'])\n",
    "#     parser.add_argument('--dataset_path', type=str, default=default_cfg['dataset_path'])\n",
    "#     parser.add_argument('--save_dir', type=str, default=default_cfg['save_dir'])\n",
    "\n",
    "#     return parser.parse_args()\n",
    "\n",
    "\n",
    "# result = parse_args()\n",
    "# print(result(args=[]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a06c07f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Namespace(epochs=10)\n"
     ]
    }
   ],
   "source": [
    "import argparse \n",
    "parser = argparse.ArgumentParser()\n",
    "\n",
    "parser.add_argument('--epochs' , type=int , default=10)\n",
    "\n",
    "args =  parser.parse_args(args=[])\n",
    "print(args) \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
