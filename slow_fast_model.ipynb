{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0163950b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch \n",
    "import cv2 as cv\n",
    "import os \n",
    "import matplotlib.pyplot as plt \n",
    "from torchinfo import summary \n",
    "import torch.nn as nn\n",
    "from typing import Tuple , Callable , Optional\n",
    "from tqdm import tqdm \n",
    "from torch.utils.data import Dataset, DataLoader , random_split\n",
    "from PIL import Image\n",
    "from torchvision import transforms "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cbbe1290",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bc9d8d23",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3179, 30)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "root_dir = '/media/ai/External/datasets/firesmoke_dataset'\n",
    "fire_vidoes_list  = os.listdir(os.path.join(root_dir , 'fire' ))\n",
    "no_fire_video_list  = os.listdir(os.path.join(root_dir , 'no_fire'))\n",
    "\n",
    "len(fire_vidoes_list) , len(os.listdir(os.path.join(root_dir,  'fire', 'firesmoke_2')))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2969384a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FireVideoDataset(Dataset):\n",
    "    def __init__(self, root_dir, chunk_size=30):\n",
    "        self.chunk_size = chunk_size\n",
    "        self.samples = []\n",
    "        self.transform = transforms.Compose([\n",
    "            transforms.ToTensor(),  # Convert PIL image to min max scale (0-1)\n",
    "            transforms.Normalize(\n",
    "                    [0.485, 0.456, 0.406],\n",
    "                    [0.229, 0.224, 0.225]\n",
    "                    ), \n",
    "            transforms.Resize((224, 224))\n",
    "            # transforms.RandomHorizontalFlip(0.5), \n",
    "            # transforms.ColorJitter(0.2)\n",
    "        ])\n",
    "        for class_name, label in [('fire', 1), ('no_fire', 0)]:\n",
    "            class_dir = os.path.join(root_dir, class_name)\n",
    "            for chunk_folder in os.listdir(class_dir):\n",
    "                chunk_path = os.path.join(class_dir, chunk_folder)\n",
    "                \n",
    "                if os.path.isdir(chunk_path) and len(os.listdir(chunk_path)) >= chunk_size:\n",
    "                    self.samples.append((chunk_path, label))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        chunk_path, label = self.samples[idx]\n",
    "        \n",
    "        image_files = os.listdir(chunk_path)[:self.chunk_size] \n",
    "        frames = []\n",
    "\n",
    "        for fname in image_files:\n",
    "            img_path = os.path.join(chunk_path, fname)\n",
    "            img = Image.open(img_path)\n",
    "\n",
    "            if img is None:\n",
    "                continue  \n",
    "            frames.append(self.transform(img)) \n",
    "        np_frame = np.array(frames)  # its [B , C , H , W]\n",
    "        np_frame = np.transpose(np_frame, (1 , 0, 2, 3))  # becomes [C , B , H , W]\n",
    "\n",
    "        return torch.tensor(np_frame, dtype=torch.float32), torch.tensor(label, dtype=torch.float32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "21b122b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = FireVideoDataset(root_dir=root_dir)\n",
    "\n",
    "train_size = int(0.7 *len(dataset))\n",
    "test_size = len(dataset) - train_size\n",
    "\n",
    "train_dataset, test_dataset = random_split(dataset, [train_size, test_size])\n",
    "\n",
    "train_data_loader = DataLoader(train_dataset, batch_size=2,shuffle=True , num_workers=8)\n",
    "test_data_loader = DataLoader(test_dataset, batch_size=2, shuffle=True , num_workers=8)\n",
    "\n",
    "# for batch_idx, (data, label) in enumerate(train_data_loader):\n",
    "#     if batch_idx == 2:\n",
    "#         break\n",
    "#     print(data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0801a83b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "210 53\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import Subset, random_split, DataLoader\n",
    "import random\n",
    "\n",
    "subset_size = int(len(dataset) * 0.1)\n",
    "\n",
    "subset_data, _ = random_split(dataset, [subset_size, len(dataset) - subset_size])\n",
    "\n",
    "train_subset_size = int(subset_size * 0.8)\n",
    "test_subset_size = subset_size - train_subset_size\n",
    "\n",
    "subset_train, subset_test = random_split(subset_data, [train_subset_size, test_subset_size])\n",
    "\n",
    "check_data_loader = DataLoader(subset_train, batch_size=2, shuffle=True, num_workers=8, drop_last=True)\n",
    "test_check_data_loader = DataLoader(subset_test, batch_size=2, shuffle=True, num_workers=8, drop_last=True)\n",
    "\n",
    "print(len(check_data_loader), len(test_check_data_loader))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3375f636",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /home/ai/.cache/torch/hub/facebookresearch_pytorchvideo_main\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "# Choose the `slowfast_r50` model \n",
    "slow_fast_model = torch.hub.load('facebookresearch/pytorchvideo', 'slowfast_r50' , pretrained=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "90c16358",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "=====================================================================================\n",
       "Layer (type:depth-idx)                                       Param #\n",
       "=====================================================================================\n",
       "Net                                                          --\n",
       "├─ModuleList: 1-1                                            --\n",
       "│    └─MultiPathWayWithFuse: 2-1                             --\n",
       "│    │    └─ModuleList: 3-1                                  15,432\n",
       "│    │    └─FuseFastToSlow: 3-2                              928\n",
       "│    └─MultiPathWayWithFuse: 2-2                             --\n",
       "│    │    └─ModuleList: 3-3                                  225,760\n",
       "│    │    └─FuseFastToSlow: 3-4                              14,464\n",
       "│    └─MultiPathWayWithFuse: 2-3                             --\n",
       "│    │    └─ModuleList: 3-5                                  1,287,552\n",
       "│    │    └─FuseFastToSlow: 3-6                              57,600\n",
       "│    └─MultiPathWayWithFuse: 2-4                             --\n",
       "│    │    └─ModuleList: 3-7                                  10,369,536\n",
       "│    │    └─FuseFastToSlow: 3-8                              229,888\n",
       "│    └─MultiPathWayWithFuse: 2-5                             --\n",
       "│    │    └─ModuleList: 3-9                                  21,443,328\n",
       "│    │    └─Identity: 3-10                                   --\n",
       "│    └─PoolConcatPathway: 2-6                                --\n",
       "│    │    └─ModuleList: 3-11                                 --\n",
       "│    └─ResNetBasicHead: 2-7                                  --\n",
       "│    │    └─Dropout: 3-12                                    --\n",
       "│    │    └─Linear: 3-13                                     922,000\n",
       "│    │    └─AdaptiveAvgPool3d: 3-14                          --\n",
       "=====================================================================================\n",
       "Total params: 34,566,488\n",
       "Trainable params: 34,566,488\n",
       "Non-trainable params: 0\n",
       "====================================================================================="
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary(slow_fast_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a6f23a02",
   "metadata": {},
   "outputs": [],
   "source": [
    "slow_fast_model = slow_fast_model.eval()\n",
    "slow_fast_model = slow_fast_model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bb10c702",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear(in_features=2304, out_features=400, bias=True)\n"
     ]
    }
   ],
   "source": [
    "print(slow_fast_model.blocks[-1].proj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a6222e30",
   "metadata": {},
   "outputs": [],
   "source": [
    "slow_fast_model.blocks[-1].proj = torch.nn.Linear(2304 , 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4c0ce312",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ResNetBasicHead(\n",
       "  (dropout): Dropout(p=0.5, inplace=False)\n",
       "  (proj): Linear(in_features=2304, out_features=1, bias=True)\n",
       "  (output_pool): AdaptiveAvgPool3d(output_size=1)\n",
       ")"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "slow_fast_model.blocks[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1808ba0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = nn.BCEWithLogitsLoss()\n",
    "def measure_acc(ypred , ytrue): \n",
    "    from torchmetrics import Accuracy , Recall\n",
    "    acc_fn = Accuracy(task='binary').to(device)\n",
    "    acc = acc_fn(ypred , ytrue)\n",
    "\n",
    "    sig_ypred = (torch.sigmoid(ypred) > 0.5).int()\n",
    "    recallfn = Recall(task='binary').to(device)\n",
    "    recall = recallfn(sig_ypred , ytrue)\n",
    "    return acc , recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06b9c16d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def slow_fast_model_train(model, train_loader:DataLoader , val_loader:DataLoader, \n",
    "                          loss_fn,\n",
    "                          number_of_epochs:int ,\n",
    "                          pathway_alpha:int)-> Tuple[list , list , list , list]:\n",
    "\n",
    "    def pack_pathway_output(frames, alpha):\n",
    "        fast_pathway = frames\n",
    "        slow_pathway = frames[:, :, ::alpha, :, :]  \n",
    "        return [slow_pathway, fast_pathway]\n",
    "    \n",
    "    def padd_to_32(data:torch.tensor) : \n",
    "            T = data.shape[2]\n",
    "            if T == 32 : \n",
    "                return data \n",
    "            elif T < 32: \n",
    "                padd = 32 - T \n",
    "                st1 =  data[:,:,-1:,:,:].repeat(1, 1, padd , 1, 1)\n",
    "                stacked = torch.cat([st1, data] , dim=2)\n",
    "                return stacked\n",
    "            else: \n",
    "                return data[:, :, :32, : , :]\n",
    "            \n",
    "\n",
    "    def transform_data(data:torch.tensor): \n",
    "\n",
    "        # transform = ApplyTransformToKey(\n",
    "        # key=\"video\",\n",
    "        # transform=Compose([\n",
    "        #     # UniformTemporalSubsample(32), # its for videos , samples are ready\n",
    "        #     Lambda(lambda x: x / 255.0),\n",
    "        #     NormalizeVideo(mean=[0.45, 0.45, 0.45], std=[0.225, 0.225, 0.225]),\n",
    "        #     ShortSideScale(256),\n",
    "        #     CenterCropVideo(224),\n",
    "        # ]))\n",
    "        \n",
    "        # input shape N, C , D , H , W \n",
    "        # p_data = data.permute(0, 2 , 1, 3 ,4)\n",
    "        # N, D, C, H, W = p_data.shape\n",
    "        # r_data = p_data.reshape(N * D, C , H, W)\n",
    "        # r_data = {'video' : r_data}\n",
    "        # t_data = transform(r_data)['video']\n",
    "        # data = t_data.reshape(N, D , C , H , W)    \n",
    "        # data = data.permute(0 , 2 , 1, 3, 4)  \n",
    "\n",
    "        data = pack_pathway_output(data , pathway_alpha)   # this transforms frame to slow and fast double frames \n",
    "        \n",
    "        return data\n",
    "        \n",
    "\n",
    "    optimizer = torch.optim.Adam(params=model.parameters(), lr=1e-3, weight_decay=1e-3)\n",
    "\n",
    "            \n",
    "    optimizer = torch.optim.Adam(params=model.parameters(), lr=1e-3)\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    model.to(device)\n",
    "\n",
    "    train_losses, val_losses = [], []\n",
    "    train_accuracies, val_accuracies = [], []\n",
    "    train_recalls , val_recalls = [] , []\n",
    "\n",
    "    for epoch in range(number_of_epochs):\n",
    "        running_loss , running_acc , running_recall = 0 , 0 , 0 \n",
    "        for x , y in tqdm(train_loader , desc=f'Epoch {epoch+1}/{number_of_epochs} - Training' ): \n",
    "            # ----- TRAINING ----- #\n",
    "            x , y = x.to(device) , y.to(device)\n",
    "            x = padd_to_32(x)\n",
    "            x = transform_data(x)\n",
    "\n",
    "            model.train() \n",
    "            ypred = model(x).squeeze()\n",
    "\n",
    "            loss = loss_fn(ypred, y)\n",
    "            optimizer.zero_grad() \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            acc , recall = measure_acc(ypred, y)\n",
    "            running_loss += loss.item() \n",
    "            running_acc  += acc \n",
    "            running_recall += recall\n",
    "        epoch_train_recall = running_recall/len(train_loader)\n",
    "        epoch_train_loss = running_loss / len(train_loader)\n",
    "        epoch_train_acc = running_acc / len(train_loader)\n",
    "        train_losses.append(epoch_train_loss)\n",
    "        train_accuracies.append(epoch_train_acc)\n",
    "        train_recalls.append(epoch_train_recall)        \n",
    "\n",
    "    # ----- EVALUATION ---- # \n",
    "\n",
    "        model.eval()\n",
    "        val_loss, val_acc , val_recall= 0.0, 0.0 , 0.0\n",
    "        with torch.inference_mode():\n",
    "            for x, y in tqdm(val_loader, desc=f\"Epoch {epoch+1}/{number_of_epochs} - Validation\"):\n",
    "                x, y = x.to(device), y.to(device)\n",
    "                x = padd_to_32(x)\n",
    "                x = transform_data(x)\n",
    "                y_pred = model(x)\n",
    "                y_pred = y_pred.squeeze()\n",
    "                loss = loss_fn(y_pred, y)\n",
    "\n",
    "                val_loss += loss.item()\n",
    "                acc, recall = measure_acc(y_pred, y)\n",
    "                val_recall += recall.item()\n",
    "                val_acc += acc.item()\n",
    "\n",
    "        epoch_val_loss = val_loss / len(val_loader)\n",
    "        epoch_val_acc = val_acc / len(val_loader)\n",
    "        epoch_val_recall = val_recall/len(val_loader)\n",
    "        val_losses.append(epoch_val_loss)\n",
    "        val_accuracies.append(epoch_val_acc)\n",
    "        val_recalls.append(epoch_val_recall)\n",
    "\n",
    "        print(f\"[Epoch {epoch+1}] Train Loss: {epoch_train_loss:.4f}, Train Acc: {epoch_train_acc:.4f} | Val Loss: {epoch_val_loss:.4f}, Val Acc: {epoch_val_acc:.4f}\")\n",
    "\n",
    "    return train_losses, train_accuracies , train_recalls, val_losses, val_accuracies , val_recall\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2bd4cf4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/10 - Training: 100%|██████████| 210/210 [04:11<00:00,  1.20s/it]\n",
      "Epoch 1/10 - Validation: 100%|██████████| 53/53 [01:04<00:00,  1.22s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 1] Train Loss: 0.6220, Train Acc: 0.6667 | Val Loss: 1.0793, Val Acc: 0.6604\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/10 - Training: 100%|██████████| 210/210 [04:01<00:00,  1.15s/it]\n",
      "Epoch 2/10 - Validation: 100%|██████████| 53/53 [01:06<00:00,  1.25s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 2] Train Loss: 0.6118, Train Acc: 0.6524 | Val Loss: 1.1883, Val Acc: 0.6132\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/10 - Training: 100%|██████████| 210/210 [04:06<00:00,  1.17s/it]\n",
      "Epoch 3/10 - Validation: 100%|██████████| 53/53 [01:05<00:00,  1.24s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 3] Train Loss: 0.6037, Train Acc: 0.6786 | Val Loss: 1.8895, Val Acc: 0.6604\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/10 - Training: 100%|██████████| 210/210 [04:10<00:00,  1.19s/it]\n",
      "Epoch 4/10 - Validation: 100%|██████████| 53/53 [01:04<00:00,  1.21s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 4] Train Loss: 0.6042, Train Acc: 0.6976 | Val Loss: 0.6112, Val Acc: 0.7358\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/10 - Training: 100%|██████████| 210/210 [03:53<00:00,  1.11s/it]\n",
      "Epoch 5/10 - Validation: 100%|██████████| 53/53 [01:04<00:00,  1.22s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 5] Train Loss: 0.5627, Train Acc: 0.7190 | Val Loss: 2.3786, Val Acc: 0.7453\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/10 - Training: 100%|██████████| 210/210 [03:49<00:00,  1.09s/it]\n",
      "Epoch 6/10 - Validation: 100%|██████████| 53/53 [01:04<00:00,  1.21s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 6] Train Loss: 0.5654, Train Acc: 0.7238 | Val Loss: 1.0564, Val Acc: 0.6604\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/10 - Training: 100%|██████████| 210/210 [03:58<00:00,  1.14s/it]\n",
      "Epoch 7/10 - Validation: 100%|██████████| 53/53 [01:02<00:00,  1.19s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 7] Train Loss: 0.5436, Train Acc: 0.7310 | Val Loss: 0.8835, Val Acc: 0.7358\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8/10 - Training: 100%|██████████| 210/210 [03:57<00:00,  1.13s/it]\n",
      "Epoch 8/10 - Validation: 100%|██████████| 53/53 [01:04<00:00,  1.22s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 8] Train Loss: 0.4799, Train Acc: 0.7810 | Val Loss: 1.2761, Val Acc: 0.5660\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9/10 - Training: 100%|██████████| 210/210 [03:58<00:00,  1.14s/it]\n",
      "Epoch 9/10 - Validation: 100%|██████████| 53/53 [01:04<00:00,  1.22s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 9] Train Loss: 0.4484, Train Acc: 0.7881 | Val Loss: 1.1631, Val Acc: 0.6604\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/10 - Training: 100%|██████████| 210/210 [03:53<00:00,  1.11s/it]\n",
      "Epoch 10/10 - Validation: 100%|██████████| 53/53 [01:00<00:00,  1.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 10] Train Loss: 0.4563, Train Acc: 0.7833 | Val Loss: 0.9451, Val Acc: 0.5943\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "train_losses, train_accuracies ,train_recalls, val_losses,val_accuracies , val_recall  = slow_fast_model_train(slow_fast_model , check_data_loader, test_check_data_loader , loss_fn, 10  , 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "cf473c51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss is 0.456286\n",
      " train accuracy is 0.78\n",
      " recall is 0.67\n",
      "val loss is 0.945058\n",
      " val accuracy is 0.59\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\n",
    "       f'train loss is {train_losses[-1]:2f}\\n',\n",
    "       f'train accuracy is {train_accuracies[-1]:.2f}\\n', \n",
    "       f'recall is {train_recalls[-1]:.2f}\\n'\n",
    "       f'val loss is {val_losses[-1]:2f}\\n',\n",
    "       f'val accuracy is {val_accuracies[-1]:.2f}\\n', \n",
    "       # f'val recall is {val_recalls[-1]:.2f}\\n'\n",
    "       )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
